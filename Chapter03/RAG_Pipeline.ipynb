{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTUldUPH_1jM"
      },
      "source": [
        "# RAG Pipeline - Data Ingestion (Context and Knowledge)\n",
        "\n",
        "Copyright 2025-2026, Denis Rothman\n",
        "\n",
        "This notebook tackles the crucial first step in building a sophisticated RAG pipeline: **data ingestion**. We're essentially setting up the \"brain\" for our agent by feeding it two distinct types of information—the procedural \"how-to\" context and the factual \"what-is\" knowledge. By the end, you'll have a fully prepped Pinecone vector database, ready to power a smarter, context-aware AI.\n",
        "\n",
        "Here’s a quick look at what this notebook does:\n",
        "* **Environment Setup:** Installs all the required Python libraries (`google gemini`, `pinecone`, `tiktoken`, etc.) and securely configures the necessary API keys.\n",
        "\n",
        "* **Vector Database Prep:** Connects to our Pinecone index, creating it if it doesn't exist and clearing out old data to ensure a clean slate.\n",
        "\n",
        "* **Procedural Context:** Defines and embeds \"Semantic Blueprints\"—our style and structure guides—and uploads them to a dedicated `ContextLibrary` namespace.\n",
        "\n",
        "* **Factual Knowledge:** Chunks raw text into optimized pieces using a tokenizer, creates embeddings for them, and uploads everything to a separate `KnowledgeStore` namespace for factual recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1bEq01K2Nmz"
      },
      "source": [
        "# 1.Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "outputs": [],
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ratho\\Desktop\\data analysis\\clone_github\\Context-Engineering-for-Multi-Agent-Systems\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9fssMtAwGlg",
        "outputId": "9ca4285f-0512-49a8-b8ec-a396b5b29e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the Google GenAI library to interact with the LLM.\n",
        "# We load the API key from a local .env file.\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "try:\n",
        "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables. Please check your .env file.\")\n",
        "\n",
        "    # Create client\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    print(\"Gemini client initialized successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
        "EMBEDDING_DIM = 3072 # Dimension for text-embedding-004\n",
        "GENERATION_MODEL = \"gemini-2.0-flash-exp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6V_5MOsBeRl",
        "outputId": "474d6e3d-a0c3-4c70-e9cb-13868b04b42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the key using the name found in your .env file\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API')\n",
        "if not PINECONE_API_KEY:\n",
        "    print(\"Warning: 'PINECONE_API' not found in environment variables. Please check your .env file.\")\n",
        "else:\n",
        "    print(\"Pinecone API key loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxctIvv62hOm"
      },
      "source": [
        "## 2.Initialize Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqAbeOskEjP4",
        "outputId": "e04b752b-6208-44ce-bc8f-a53c3b8df394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 'genai-mas-mcp-ch3' not found. Creating new serverless index...\n",
            "Index created successfully. It is new and empty.\n"
          ]
        }
      ],
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM,\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    print(f\"Index '{INDEX_NAME}' already exists. Clearing namespaces for a fresh start...\")\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "    namespaces_to_clear = [NAMESPACE_KNOWLEDGE, NAMESPACE_CONTEXT]\n",
        "\n",
        "    for namespace in namespaces_to_clear:\n",
        "        # Check if namespace exists and has vectors before deleting\n",
        "        stats = index.describe_index_stats()\n",
        "        if namespace in stats.namespaces and stats.namespaces[namespace].vector_count > 0:\n",
        "            print(f\"Clearing namespace '{namespace}'...\")\n",
        "            index.delete(delete_all=True, namespace=namespace)\n",
        "\n",
        "            # **CRITICAL FUNCTTION: Wait for deletion to complete**\n",
        "            while True:\n",
        "                stats = index.describe_index_stats()\n",
        "                if namespace not in stats.namespaces or stats.namespaces[namespace].vector_count == 0:\n",
        "                    print(f\"Namespace '{namespace}' cleared successfully.\")\n",
        "                    break\n",
        "                print(f\"Waiting for namespace '{namespace}' to clear...\")\n",
        "                time.sleep(5) # Poll every 5 seconds\n",
        "        else:\n",
        "            print(f\"Namespace '{namespace}' is already empty or does not exist. Skipping.\")\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXkeOtzx2Ws-"
      },
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XaDNRzlGP1P",
        "outputId": "2ad9b463-e895-4140-ab07-e19880333469"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prepared 3 context blueprints.\n"
          ]
        }
      ],
      "source": [
        "# 3.Data Preparation: The Context Library (Procedural RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the Semantic Blueprints derived from Chapter 1.\n",
        "# CRITICAL: We embed the 'description' (the intent), so the Librarian agent\n",
        "# can find the right blueprint based on the desired style. The 'blueprint'\n",
        "# itself is stored as metadata.\n",
        "\n",
        "context_blueprints = [\n",
        "    {\n",
        "        \"id\": \"blueprint_suspense_narrative\",\n",
        "        \"description\": \"A precise Semantic Blueprint designed to generate suspenseful and tense narratives, suitable for children's stories. Focuses on atmosphere, perceived threats, and emotional impact. Ideal for creative writing.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Increase tension and create suspense.\",\n",
        "              \"style_guide\": \"Use short, sharp sentences. Focus on sensory details (sounds, shadows). Maintain a slightly eerie but age-appropriate tone.\",\n",
        "              \"participants\": [\n",
        "                { \"role\": \"Agent\", \"description\": \"The protagonist experiencing the events.\" },\n",
        "                { \"role\": \"Source_of_Threat\", \"description\": \"The underlying danger or mystery.\" }\n",
        "              ],\n",
        "            \"instruction\": \"Rewrite the provided facts into a narrative adhering strictly to the scene_goal and style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_technical_explanation\",\n",
        "        \"description\": \"A Semantic Blueprint designed for technical explanation or analysis. This blueprint focuses on clarity, objectivity, and structure. Ideal for breaking down complex processes, explaining mechanisms, or summarizing scientific findings.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Explain the mechanism or findings clearly and concisely.\",\n",
        "              \"style_guide\": \"Maintain an objective and formal tone. Use precise terminology. Prioritize factual accuracy and clarity over narrative flair.\",\n",
        "              \"structure\": [\"Definition\", \"Function/Operation\", \"Key Findings/Impact\"],\n",
        "              \"instruction\": \"Organize the provided facts into the defined structure, adhering to the style_guide.\"\n",
        "            })\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"blueprint_casual_summary\",\n",
        "        \"description\": \"A goal-oriented context for creating a casual, easy-to-read summary. Focuses on brevity and accessibility, explaining concepts simply.\",\n",
        "        \"blueprint\": json.dumps({\n",
        "              \"scene_goal\": \"Summarize information quickly and casually.\",\n",
        "              \"style_guide\": \"Use informal language. Keep it brief and engaging. Imagine explaining it to a friend.\",\n",
        "              \"instruction\": \"Summarize the provided facts using the casual style guide.\"\n",
        "            })\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"\\nPrepared {len(context_blueprints)} context blueprints.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GitdjWeYGrum"
      },
      "outputs": [],
      "source": [
        "#@title 4.Data Preparation: The Knowledge Base (Factual RAG)\n",
        "# -------------------------------------------------------------------------\n",
        "# We use sample data related to space exploration.\n",
        "\n",
        "knowledge_data_raw = \"\"\"\n",
        "Space exploration is the use of astronomy and space technology to explore outer space. The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. The launch of the Soviet Union's Sputnik 1 in 1957, and the first Moon landing by the American Apollo 11 mission in 1969 are key landmarks.\n",
        "\n",
        "The Apollo program was the United States human spaceflight program carried out by NASA which succeeded in landing the first humans on the Moon. Apollo 11 was the first mission to land on the Moon, commanded by Neil Armstrong and lunar module pilot Buzz Aldrin, with Michael Collins as command module pilot. Armstrong's first step onto the lunar surface occurred on July 20, 1969, and was broadcast on live TV worldwide. The landing required Armstrong to take manual control of the Lunar Module Eagle due to navigational challenges and low fuel.\n",
        "\n",
        "Juno is a NASA space probe orbiting the planet Jupiter. It was launched on August 5, 2011, and entered a polar orbit of Jupiter on July 5, 2016. Juno's mission is to measure Jupiter's composition, gravitational field, magnetic field, and polar magnetosphere to understand how the planet formed. Juno is the second spacecraft to orbit Jupiter, after the Galileo orbiter. It is uniquely powered by large solar arrays instead of RTGs (Radioisotope Thermoelectric Generators), making it the farthest solar-powered mission.\n",
        "\n",
        "A Mars rover is a remote-controlled motor vehicle designed to travel on the surface of Mars. NASA JPL managed several successful rovers including: Sojourner, Spirit, Opportunity, Curiosity, and Perseverance. The search for evidence of habitability and organic carbon on Mars is now a primary NASA objective. Perseverance also carried the Ingenuity helicopter.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5qqwP4AfG0ZW"
      },
      "outputs": [],
      "source": [
        "#@title 5.Helper Functions for Chunking and Embedding\n",
        "# -------------------------------------------------------------------------\n",
        "import tiktoken\n",
        "# Initialize tokenizer for robust, token-aware chunking\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    \"\"\"Chunks text based on token count with overlap (Best practice for RAG).\"\"\"\n",
        "    tokens = tokenizer.encode(text)\n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), chunk_size - overlap):\n",
        "        chunk_tokens = tokens[i:i + chunk_size]\n",
        "        chunk_text = tokenizer.decode(chunk_tokens)\n",
        "        # Basic cleanup\n",
        "        chunk_text = chunk_text.replace(\"\\n\", \" \").strip()\n",
        "        if chunk_text:\n",
        "            chunks.append(chunk_text)\n",
        "    return chunks\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embeddings_batch(texts, model=EMBEDDING_MODEL):\n",
        "    \"\"\"Generates embeddings for a batch of texts using Gemini, with retries.\"\"\"\n",
        "    # Gemini expects the input texts to have newlines replaced by spaces\n",
        "    texts = [t.replace(\"\\n\", \" \") for t in texts]\n",
        "    \n",
        "    # REMOVED: config=types.EmbedContentConfig(...) because gemini-embedding-001 \n",
        "    # does not support output_dimensionality configuration.\n",
        "    response = client.models.embed_content(\n",
        "        model=model,\n",
        "        contents=texts\n",
        "    )\n",
        "    return [item.values for item in response.embeddings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1nYQq4M8_vP8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing and uploading Context Library to namespace: ContextLibrary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3/3 [00:01<00:00,  2.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully uploaded 3 context vectors.\n",
            "\n",
            "Processing and uploading Knowledge Base to namespace: KnowledgeStore\n",
            "Created 2 knowledge chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully uploaded 2 knowledge vectors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@title 6.Process and Upload Data\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# --- 6.1. Context Library ---\n",
        "print(f\"\\nProcessing and uploading Context Library to namespace: {NAMESPACE_CONTEXT}\")\n",
        "\n",
        "vectors_context = []\n",
        "for item in tqdm(context_blueprints):\n",
        "    # We embed the DESCRIPTION (the intent)\n",
        "    embedding = get_embeddings_batch([item['description']])[0]\n",
        "    vectors_context.append({\n",
        "        \"id\": item['id'],\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": {\n",
        "            \"description\": item['description'],\n",
        "            # The blueprint itself (JSON string) is stored as metadata\n",
        "            \"blueprint_json\": item['blueprint']\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Upsert data\n",
        "if vectors_context:\n",
        "    index.upsert(vectors=vectors_context, namespace=NAMESPACE_CONTEXT)\n",
        "    print(f\"Successfully uploaded {len(vectors_context)} context vectors.\")\n",
        "\n",
        "# --- 6.2. Knowledge Base ---\n",
        "print(f\"\\nProcessing and uploading Knowledge Base to namespace: {NAMESPACE_KNOWLEDGE}\")\n",
        "\n",
        "# Chunk the knowledge data\n",
        "knowledge_chunks = chunk_text(knowledge_data_raw)\n",
        "print(f\"Created {len(knowledge_chunks)} knowledge chunks.\")\n",
        "\n",
        "vectors_knowledge = []\n",
        "batch_size = 100 # Process in batches\n",
        "\n",
        "for i in tqdm(range(0, len(knowledge_chunks), batch_size)):\n",
        "    batch_texts = knowledge_chunks[i:i+batch_size]\n",
        "    batch_embeddings = get_embeddings_batch(batch_texts)\n",
        "\n",
        "    batch_vectors = []\n",
        "    for j, embedding in enumerate(batch_embeddings):\n",
        "        chunk_id = f\"knowledge_chunk_{i+j}\"\n",
        "        batch_vectors.append({\n",
        "            \"id\": chunk_id,\n",
        "            \"values\": embedding,\n",
        "            \"metadata\": {\n",
        "                \"text\": batch_texts[j]\n",
        "            }\n",
        "        })\n",
        "    # Upsert the batch\n",
        "    index.upsert(vectors=batch_vectors, namespace=NAMESPACE_KNOWLEDGE)\n",
        "\n",
        "print(f\"Successfully uploaded {len(knowledge_chunks)} knowledge vectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uF27i5EHLjI",
        "outputId": "50fc2bc6-7cb6-4ffb-924b-58592e70aa83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ingestion complete. Final Pinecone Index Stats (may take a moment to update):\n",
            "{'dimension': 3072,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'ContextLibrary': {'vector_count': 3},\n",
            "                'KnowledgeStore': {'vector_count': 2}},\n",
            " 'total_vector_count': 5,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ],
      "source": [
        "#@title 7.Final Verification\n",
        "# -------------------------------------------------------------------------\n",
        "print(\"\\nIngestion complete. Final Pinecone Index Stats (may take a moment to update):\")\n",
        "time.sleep(15) # Give Pinecone a moment to update stats\n",
        "print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Model: models/gemini-embedding-001\n",
            "Actual Vector output dimension: 3072\n"
          ]
        }
      ],
      "source": [
        "# --- CHECK DIMENSION ---\n",
        "print(f\"Current Model: {EMBEDDING_MODEL}\")\n",
        "try:\n",
        "    test_vec = get_embeddings_batch([\"test\"])[0]\n",
        "    print(f\"Actual Vector output dimension: {len(test_vec)}\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMwHK5PxmxF1Yq4fHyA11vN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
