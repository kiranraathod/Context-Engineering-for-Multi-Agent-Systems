{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzTVaRXb/KrMQOytwKsqtO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Context-Aware MAS Implementation\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "This notebook implements an educational MAS architecture utilizing RAG via Pinecone with MCP.\n",
        "\n",
        "This notebook implements the execution layer of our Context-Aware system. We'll build a complete Multi-Agent System (MAS) where specialized agents collaborate to fulfill a high-level goal, putting the data we previously ingested into Pinecone to work. The core architecture is designed to cleanly separate procedural instructions (the how) from factual data (the what), enabling highly flexible and controlled content generation.\n",
        "\n",
        "Here's a breakdown of the plan:\n",
        "\n",
        "Agent Definitions: We will code three specialized agents that form the core of the system:\n",
        "\n",
        "The Context Librarian performs procedural RAG to fetch stylistic \"Semantic Blueprints.\"\n",
        "\n",
        "The Researcher uses factual RAG to retrieve and synthesize knowledge on a given topic.\n",
        "\n",
        "The Writer intelligently combines the blueprint and the research to generate the final output.\n",
        "\n",
        "The Orchestrator: This agent acts as the manager. It uses an LLM to analyze a user's goal, breaking it down into distinct intent and topic queries for the other agents.\n",
        "\n",
        "Agent Communication: A simple Message Communication Protocol (MCP) is defined to ensure agents interact in a structured and traceable way.\n",
        "\n",
        "End-to-End Execution: We'll run several examples to demonstrate how the MAS can generate unique outputs for various topics by dynamically retrieving the correct context and knowledge."
      ],
      "metadata": {
        "id": "e6NskRP-IwEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e415f0-afb2-49e1-8eba-672d6cddfbbb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==2.8.1\n",
            "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.8.1) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.8.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.4.2)\n",
            "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-2.8.1\n",
            "Collecting pinecone==7.0.0\n",
            "  Downloading pinecone-7.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting tenacity==8.3.0\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.10.5)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone==7.0.0)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n",
            "Downloading pinecone-7.0.0-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: tenacity, pinecone-plugin-interface, pinecone\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 8.5.0\n",
            "    Uninstalling tenacity-8.5.0:\n",
            "      Successfully uninstalled tenacity-8.5.0\n",
            "Successfully installed pinecone-7.0.0 pinecone-plugin-interface-0.0.7 tenacity-8.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5.1\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64de6e7a-e0a0-4eeb-9510-d640b6404587"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d1efcf1-1f88-4df2-bd94-fb27b334cd43"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)\n",
        "\n",
        "# Connect to the index for subsequent operations\n",
        "index = pc.Index(INDEX_NAME)\n"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70911b87-611c-4ef1-ab8c-32a469130089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Helper Functions (LLM, Embeddings, and MCP)\n",
        "# -------------------------------------------------------------------------\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm(system_prompt, user_prompt, temperature=1, json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        response = client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        return f\"LLM Error: {e}\"\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message (Educational Version).\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"1.1 (RAG-Enhanced)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content,\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "def display_mcp(message, title=\"MCP Message\"):\n",
        "    \"\"\"Helper function to display MCP messages clearly during the trace.\"\"\"\n",
        "    print(f\"\\n--- {title} (Sender: {message['sender']}) ---\")\n",
        "    # Display content snippet or keys if content is complex\n",
        "    if isinstance(message['content'], dict):\n",
        "         print(f\"Content Keys: {list(message['content'].keys())}\")\n",
        "    else:\n",
        "        print(f\"Content: {textwrap.shorten(str(message['content']), width=100)}\")\n",
        "    # Display metadata keys\n",
        "    print(f\"Metadata Keys: {list(message['metadata'].keys())}\")\n",
        "    print(\"-\" * (len(title) + 25))\n",
        "\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Helper functions and MCP structure defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5xqzbgK6vS",
        "outputId": "8b500194-a2e2-4020-f892-f973b6762980"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions and MCP structure defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.Agent Definitions\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    requested_intent = mcp_message['content']['intent_query']\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        content = {\"blueprint\": blueprint_json}\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = {\"blueprint\": json.dumps({\"instruction\": \"Generate the content neutrally.\"})}\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    topic = mcp_message['content']['topic_query']\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        return create_mcp_message(\"Researcher\", {\"facts\": \"No data found.\"})\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    findings = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Researcher\", {\"facts\": findings})\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to facts...\")\n",
        "\n",
        "    facts = mcp_message['content']['facts']\n",
        "    # The blueprint is passed as a JSON string\n",
        "    blueprint_json_string = mcp_message['content']['blueprint']\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided RESEARCH FINDINGS.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the research defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- RESEARCH FINDINGS ---\n",
        "    {facts}\n",
        "    --- END RESEARCH FINDINGS ---\n",
        "\n",
        "    Generate the content now.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm(system_prompt, user_prompt)\n",
        "\n",
        "    return create_mcp_message(\"Writer\", {\"output\": final_output})"
      ],
      "metadata": {
        "id": "MGu0zBuGLKnI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.The Orchestrator\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "def orchestrator(high_level_goal):\n",
        "    \"\"\"\n",
        "    Manages the workflow of the Context-Aware MAS.\n",
        "    Analyzes the goal, retrieves context and facts, and coordinates generation.\n",
        "    \"\"\"\n",
        "    print(f\"=== [Orchestrator] Starting New Task ===\")\n",
        "    print(f\"Goal: {high_level_goal}\")\n",
        "\n",
        "    # Step 0: Analyze Goal (Determine Intent and Topic)\n",
        "    # We use the LLM to separate the desired style (intent) from the subject matter (topic).\n",
        "    print(\"\\n[Orchestrator] Analyzing Goal...\")\n",
        "    analysis_system_prompt = \"\"\"You are an expert goal analyst. Analyze the user's high-level goal and extract two components:\n",
        "    1. 'intent_query': A descriptive phrase summarizing the desired style, tone, or format, optimized for searching a context library (e.g., \"suspenseful narrative blueprint\", \"objective technical explanation structure\").\n",
        "    2. 'topic_query': A concise phrase summarizing the factual subject matter required (e.g., \"Juno mission objectives and power\", \"Apollo 11 landing details\").\n",
        "\n",
        "    Respond ONLY with a JSON object containing these two keys.\"\"\"\n",
        "\n",
        "    # We request JSON mode for reliable parsing\n",
        "    analysis_result = call_llm(analysis_system_prompt, high_level_goal, json_mode=True)\n",
        "\n",
        "    try:\n",
        "        analysis = json.loads(analysis_result)\n",
        "        intent_query = analysis['intent_query']\n",
        "        topic_query = analysis['topic_query']\n",
        "    except (json.JSONDecodeError, KeyError):\n",
        "        print(f\"[Orchestrator] Error: Could not parse analysis JSON. Raw Analysis: {analysis_result}. Aborting.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Orchestrator: Intent Query: '{intent_query}'\")\n",
        "    print(f\"Orchestrator: Topic Query: '{topic_query}'\")\n",
        "\n",
        "\n",
        "    # Step 1: Get the Context Blueprint (Procedural RAG)\n",
        "    mcp_to_librarian = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"intent_query\": intent_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_librarian, \"Orchestrator -> Librarian\")\n",
        "    mcp_from_librarian = agent_context_librarian(mcp_to_librarian)\n",
        "    display_mcp(mcp_from_librarian, \"Librarian -> Orchestrator\")\n",
        "\n",
        "    context_blueprint = mcp_from_librarian['content'].get('blueprint')\n",
        "    if not context_blueprint: return\n",
        "\n",
        "    # Step 2: Get the Factual Knowledge (Factual RAG)\n",
        "    mcp_to_researcher = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content={\"topic_query\": topic_query}\n",
        "    )\n",
        "    # display_mcp(mcp_to_researcher, \"Orchestrator -> Researcher\")\n",
        "    mcp_from_researcher = agent_researcher(mcp_to_researcher)\n",
        "    display_mcp(mcp_from_researcher, \"Researcher -> Orchestrator\")\n",
        "\n",
        "    research_findings = mcp_from_researcher['content'].get('facts')\n",
        "    if not research_findings: return\n",
        "\n",
        "    # Step 3: Generate the Final Output\n",
        "    # Combine the outputs for the Writer Agent\n",
        "    writer_task = {\n",
        "        \"blueprint\": context_blueprint,\n",
        "        \"facts\": research_findings\n",
        "    }\n",
        "\n",
        "    mcp_to_writer = create_mcp_message(\n",
        "        sender=\"Orchestrator\",\n",
        "        content=writer_task\n",
        "    )\n",
        "    # display_mcp(mcp_to_writer, \"Orchestrator -> Writer\")\n",
        "    mcp_from_writer = agent_writer(mcp_to_writer)\n",
        "    display_mcp(mcp_from_writer, \"Writer -> Orchestrator\")\n",
        "\n",
        "    final_result = mcp_from_writer['content'].get('output')\n",
        "\n",
        "    print(\"\\n=== [Orchestrator] Task Complete ===\")\n",
        "    return final_result"
      ],
      "metadata": {
        "id": "kPEcJnBwLUQc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.Running examples\n"
      ],
      "metadata": {
        "id": "NcptoMmoLwKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 1: Requesting a specific style (Suspense) for a topic (Apollo 11)\n",
        "print(\"********  1: SUSPENSEFUL NARRATIVE **********\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "result_1 = orchestrator(goal_1)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "print(result_1)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VclgEP1qLp5l",
        "outputId": "c6c090f9-7915-4fa9-8d64-0c3c97e20cf8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********  1: SUSPENSEFUL NARRATIVE **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'short suspenseful children's story scene structure'\n",
            "Orchestrator: Topic Query: 'Apollo 11 moon landing dangers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.57)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n",
            "The cabin hums.\n",
            "\n",
            "A thin, constant vibration. Like a held breath made of metal and wires.\n",
            "\n",
            "The Moon hangs below, a gray curve in the black. Too close. Not close enough.\n",
            "\n",
            "“Program alarm.”\n",
            "\n",
            "The words cut through the headset. Sharp. Flat. The numbers on the display jump, freeze, jump again. Something is wrong with the guidance computer. It’s thinking too hard. Or not at all.\n",
            "\n",
            "Armstrong feels the shift before anyone explains it. A pause in the machine’s confidence. He stares at the numbers. At the window. At the dusted surface rising up to meet them.\n",
            "\n",
            "The descent is supposed to be automatic.\n",
            "\n",
            "It isn’t anymore.\n",
            "\n",
            "Outside, the shadows crawl over craters and boulders. The landing site, the one they planned and plotted and studied for months, slips past beneath them. The computer is guiding them toward a field of rocks. Big ones. Jagged. Deadly.\n",
            "\n",
            "He switches to manual.\n",
            "\n",
            "His hand closes around the control stick. It feels too small. Too light. The Moon moves in the window, slow and slippery, as if it might slide away if he doesn’t hold it still.\n",
            "\n",
            "Fuel ticks down.\n",
            "\n",
            "Aldrin calls out numbers. Altitude. Velocity. Fuel. His voice is tight, but steady. The cabin is cramped, hot, full of the smell of plastic, metal, sweat. Every sound is too loud. Every silence worse.\n",
            "\n",
            "“Thirty seconds.”\n",
            "\n",
            "Thirty seconds until the fuel is gone. Until the engines die and gravity decides everything.\n",
            "\n",
            "Armstrong nudges the ship forward. Then down. Eyes flicking from the window to the instruments to the dust, searching for something flat, something safe, something not broken and sharp and waiting to tear them apart.\n",
            "\n",
            "Dust begins to stir below. A faint gray mist kicked up by the engine. It curls and swirls, hiding the rocks, hiding the truth of the ground. He has to see through it. Guess through it.\n",
            "\n",
            "The engine roars in the bones of the craft.\n",
            "\n",
            "Somewhere far behind them, alone in lunar orbit, Collins circles. A silent guardian in the command module. If they don’t land safely, if they don’t lift off again, he will pass over this spot again and again, the only living man in the sky, carrying the way home that no one can reach.\n",
            "\n",
            "“Contact light.”\n",
            "\n",
            "The words snap out. A tiny blue light glows. The Moon has touched them. Or they have touched the Moon. It’s hard to tell which.\n",
            "\n",
            "Armstrong cuts the engine.\n",
            "\n",
            "The world goes strangely quiet. No roar. Just the soft ticks and clicks of systems adjusting to stillness. He feels the craft settle into the dust. A whispering stop, as if the Moon is trying to decide whether to keep them.\n",
            "\n",
            "They are down.\n",
            "\n",
            "They are not safe.\n",
            "\n",
            "Above, billions of eyes stare at glowing screens. Living rooms lit by a ghostly gray glow. Every face turned toward a flickering image of a ladder that leads into darkness.\n",
            "\n",
            "If something fails now, they will all see it. The suit could rip. The line to the cabin could fail. The engine for the return could die, trapping them in the cold. Any mistake will not be quiet. Not hidden. The entire world is watching.\n",
            "\n",
            "The hatch opens with a muffled clank.\n",
            "\n",
            "Blackness waits outside. Not empty—just full of things that don’t make sound. Dust. Rock. The pull of a smaller gravity that tugs at everything, including courage.\n",
            "\n",
            "Armstrong backs down the ladder. One rung. Then another. Gloved hands feel for edges he cannot see. The suit is stiff. Heavy. Every move is slow, deliberate, as if the air itself has turned to glass.\n",
            "\n",
            "His boot hovers over the last rung.\n",
            "\n",
            "Below, the Moon’s dust lies untouched. Pale. Powdery. Cold. It almost seems to move, breathing in tiny clouds in the harsh glare of sunlight and shadow. No wind. No sound. Just the silent waiting of a world that has never known footsteps.\n",
            "\n",
            "He lowers his foot.\n",
            "\n",
            "For a heartbeat, nothing happens.\n",
            "\n",
            "Then his boot presses into the dust. It gives way easily, swallowing the tread pattern, holding it, recording it. A mark that will not fade for ages. A human shape stamped into a place that has never known humans.\n",
            "\n",
            "His voice travels up through the suit, through the microphone, up into the antenna, across the emptiness, down to the spinning Earth. Into living rooms. Into bedrooms. Into crowded bars and quiet kitchens.\n",
            "\n",
            "Everyone hears him.\n",
            "\n",
            "Everyone sees him.\n",
            "\n",
            "But up here, it feels different. It feels like the Moon is listening too.\n",
            "\n",
            "In the distance, the land stretches into black shadow. Craters like open mouths. Sharp ridges cutting the light. Somewhere under that silence lies the danger that has followed them all this way: one failed system, one broken valve, one miscalculation.\n",
            "\n",
            "They walk. They work. They collect dust and rocks under the harsh, unblinking sunlight. Shadows cling to their boots, hard-edged and too dark. The radio crackles with checklists and confirmations, but beneath the words lies another layer: the quiet knowledge that any small failure could strand them forever within those shadows.\n",
            "\n",
            "The world watches every step.\n",
            "\n",
            "The real test waits later.\n",
            "\n",
            "The ascent engine. The tiny spark that must lift them off this gray, dead sea and up into Collins’ orbiting path. It has never been fired from a surface like this before. If it doesn’t light, nothing can help.\n",
            "\n",
            "For now, though, Armstrong’s footprints deepen. Aldrin’s follow, crisscrossing, tracing a fragile pattern of human presence. Every mark in the dust is a promise—and a question.\n",
            "\n",
            "They came this far.\n",
            "\n",
            "Can they get back?\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example 2: Requesting a different style (Technical) for another topic (Juno)\n",
        "print(\"******** 2: TECHNICAL EXPLANATION **********\")\n",
        "goal_2 = \"Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\"\n",
        "result_2 = orchestrator(goal_2)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 2 **********\\n\")\n",
        "print(result_2)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij6p9Ph5MCiC",
        "outputId": "13ba1ae9-7a26-4981-92a1-9d95fb2bc2fd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 2: TECHNICAL EXPLANATION **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Provide a clear technical explanation of the Juno mission's objectives and how it is powered.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'clear technical explanation structure'\n",
            "Orchestrator: Topic Query: 'Juno mission objectives and power source'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_technical_explanation' (Score: 0.49)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 2 **********\n",
            "\n",
            "**Definition**  \n",
            "Juno is a NASA space probe designed to study the planet Jupiter from orbit. Launched on August 5, 2011, it entered a polar orbit around Jupiter on July 5, 2016. It is the second spacecraft to orbit Jupiter, following the Galileo orbiter.\n",
            "\n",
            "**Function/Operation**  \n",
            "Juno operates in a highly elliptical polar orbit around Jupiter, enabling repeated passes over the planet’s poles and through its radiation environment. Its primary scientific functions are to measure Jupiter’s composition, gravitational field, magnetic field, and polar magnetosphere. The spacecraft is powered by large solar arrays rather than radioisotope thermoelectric generators, an operational choice that enables sustained power generation at Jupiter’s distance from the Sun.\n",
            "\n",
            "**Key Findings/Impact**  \n",
            "By combining measurements of composition, gravity, magnetic fields, and polar magnetospheric activity, Juno provides data that constrain models of Jupiter’s internal structure and origin. These observations are central to understanding how Jupiter formed and, by extension, contribute to broader theories of giant planet formation and the early evolution of the solar system. The mission also demonstrates the viability of solar power at large heliocentric distances, establishing Juno as the farthest solar-powered mission to date and influencing design considerations for future deep-space spacecraft.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Example 3: Requesting a casual style\n",
        "print(\"******** 3: CASUAL SUMMARY **********\")\n",
        "goal_3 = \"Give me a quick, casual summary of what Mars rovers do.\"\n",
        "result_3 = orchestrator(goal_3)\n",
        "\n",
        "print(\"\\n******** FINAL OUTPUT 3 **********\\n\")\n",
        "print(result_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkRChqNiMJOx",
        "outputId": "40b0a4fc-1a6d-4591-de88-8b4cec9f78f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** 3: CASUAL SUMMARY **********\n",
            "=== [Orchestrator] Starting New Task ===\n",
            "Goal: Give me a quick, casual summary of what Mars rovers do.\n",
            "\n",
            "[Orchestrator] Analyzing Goal...\n",
            "Orchestrator: Intent Query: 'quick casual explanatory summary'\n",
            "Orchestrator: Topic Query: 'functions and activities of Mars rovers'\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_casual_summary' (Score: 0.64)\n",
            "\n",
            "--- Librarian -> Orchestrator (Sender: Librarian) ---\n",
            "Content Keys: ['blueprint']\n",
            "Metadata Keys: []\n",
            "--------------------------------------------------\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "\n",
            "--- Researcher -> Orchestrator (Sender: Researcher) ---\n",
            "Content Keys: ['facts']\n",
            "Metadata Keys: []\n",
            "---------------------------------------------------\n",
            "\n",
            "[Writer] Activated. Applying blueprint to facts...\n",
            "\n",
            "--- Writer -> Orchestrator (Sender: Writer) ---\n",
            "Content Keys: ['output']\n",
            "Metadata Keys: []\n",
            "-----------------------------------------------\n",
            "\n",
            "=== [Orchestrator] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 3 **********\n",
            "\n",
            "Mars rovers are basically NASA’s robot cars on Mars. They’re remote-controlled from Earth and built to roll around the planet, check out different rocks and landscapes, and help answer the big question: could Mars have ever supported life?\n",
            "\n",
            "NASA’s Jet Propulsion Lab has sent a whole lineup over the years: Sojourner, Spirit, Opportunity, Curiosity, and Perseverance. Each one upgraded the tech and the science a bit more.\n",
            "\n",
            "Their main jobs:\n",
            "- Drive across the surface and explore new terrain  \n",
            "- Run experiments and use instruments controlled by teams on Earth  \n",
            "- Look for signs of habitability and organic carbon (stuff related to life)\n",
            "\n",
            "Perseverance is the newest and flashiest one. It’s focused on finding evidence of past habitability and organic molecules. It also brought along the Ingenuity helicopter, which was the first aircraft to fly on another planet, helping scout the terrain from the air.\n"
          ]
        }
      ]
    }
  ]
}