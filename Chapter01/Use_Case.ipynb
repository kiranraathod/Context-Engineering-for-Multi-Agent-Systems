{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Meeting Analysis Use Case\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "The notebook's goal is to demonstrate how to guide an AI through a multi-step analytical process, moving from a raw transcript to actionable insights, thereby training both the user and the AI.\n",
        "\n",
        "This notebook is a step-by-step guide on how you can engineer a structured context . This guide will serve as the architectural blueprint for your code, explaining not just the *what* but the *why* at each stage.\n"
      ],
      "metadata": {
        "id": "FDBDBUKfSggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Setup and Preliminaries\n",
        "\n",
        "This section handles the basic configuration.\n",
        "\n",
        "1.  **Cell 1: Install Libraries**\n",
        "\n",
        "      * Install the necessary OpenAI library."
      ],
      "metadata": {
        "id": "ivJQbp_mwtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Installation\n",
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting openai==2.8.1\n",
            "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.8.1) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.8.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.4.2)\n",
            "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.109.1\n",
            "    Uninstalling openai-1.109.1:\n",
            "      Successfully uninstalled openai-1.109.1\n",
            "Successfully installed openai-2.8.1\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "voHBQKGtSggF",
        "outputId": "82812451-5788-47c8-c3b6-640894ed9f5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 2: Imports and API Key**\n",
        "\n",
        "      * Import the library and securely prompt for the user's API key. This is better than hard-coding it."
      ],
      "metadata": {
        "id": "jtGvDLpLSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "eNHm5qkhSggG",
        "outputId": "7ada89c2-0ea6-48a4-d26b-37dc63e7fa76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Cell 3: The Raw Data (The \"Crime Scene\")**\n",
        "\n",
        "      * Define the meeting transcript as a multi-line string. This is our primary data source."
      ],
      "metadata": {
        "id": "Vgp62z7OSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: The Full Meeting Transcript\n",
        "meeting_transcript = \"\"\"\n",
        "        Tom: Morning all. Coffee is still kicking in.\n",
        "        Sarah: Morning, Tom. Right, let's jump in. Project Phoenix timeline. Tom, you said the backend components are on track?\n",
        "        Tom: Mostly. We hit a small snag with the payment gateway integration. It's... more complex than the docs suggested. We might need another three days.\n",
        "        Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.\n",
        "        Sarah: I agree with Maria. What's the alternative, Tom?\n",
        "        Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.\n",
        "        Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?\n",
        "        Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.\n",
        "        Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?\n",
        "        Tom: Seems standard.\n",
        "        Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.\n",
        "        Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n",
        "        Tom: Sounds good. Now, more coffee.\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "UJpZUVXpSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 1: Establishing the scope (The 'What')\n",
        "\n",
        "Here, we define the scope of the analysis. Each step's output will inform the next, creating a chain of context.\n",
        "\n",
        "1.  **Cell 4: g2 - Set the Signal-to-Noise Ratio**\n",
        "\n",
        "      * We'll start by cleaning the data. The prompt explicitly tells the AI to separate substantive content from conversational noise."
      ],
      "metadata": {
        "id": "40PTjmLaSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: g2 - Isolating Content from Noise\n",
        "prompt_g2 = f\"\"\"\n",
        "        Analyze the following meeting transcript. Your task is to isolate the substantive content from the conversational noise.\n",
        "        - Substantive content includes: decisions made, project updates, problems raised, and strategic suggestions.\n",
        "        - Noise includes: greetings, pleasantries, and off-topic remarks (like coffee).\n",
        "        Return ONLY the substantive content.\n",
        "\n",
        "        Transcript:\n",
        "        ---\n",
        "        {meeting_transcript}\n",
        "        ---\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "069eAXt9SggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    client = OpenAI()\n",
        "\n",
        "    response_g2 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_g2}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    substantive_content = response_g2.choices[0].message.content\n",
        "    print(\"--- SUBSTANTIVE CONTENT ---\")\n",
        "    print(substantive_content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ptfVZHlps3r4",
        "outputId": "0aeeb5d3-7dde-4b8e-f4d3-dfa2adf3949b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SUBSTANTIVE CONTENT ---\n",
            "Sarah: Project Phoenix timeline. Tom, you said the backend components are on track?  \n",
            "Tom: Mostly. We hit a small snag with the payment gateway integration. It's more complex than the docs suggested. We might need another three days.  \n",
            "Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.  \n",
            "Sarah: I agree with Maria. What's the alternative, Tom?  \n",
            "Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.  \n",
            "Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?  \n",
            "Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.  \n",
            "Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?  \n",
            "Tom: Seems standard.  \n",
            "Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.  \n",
            "Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 5: g3 - Define the Scope of Time (Simulated RAG)**\n",
        "\n",
        "      * We'll simulate a RAG context by providing a \"previous\" summary and asking for what's new. This teaches the user the importance of historical context."
      ],
      "metadata": {
        "id": "XY2ZlN8sSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: g3 - Identifying NEW Information (Simulated RAG)\n",
        "previous_summary = \"In our last meeting, we finalized the goals for Project Phoenix and assigned backend work to Tom and front-end to Maria.\"\n",
        "\n",
        "prompt_g3 = f\"\"\"\n",
        "Context: The summary of our last meeting was: \"{previous_summary}\"\n",
        "\n",
        "Task: Analyze the following substantive content from our new meeting. Identify and summarize ONLY the new developments, problems, or decisions that have occurred since the last meeting.\n",
        "\n",
        "New Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your chat completion request\n",
        "try:\n",
        "    response_g3 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g3}]\n",
        "    )\n",
        "    new_developments = response_g3.choices[0].message.content\n",
        "    print(\"--- NEW DEVELOPMENTS SINCE LAST MEETING ---\")\n",
        "    print(new_developments)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NEW DEVELOPMENTS SINCE LAST MEETING ---\n",
            "New developments, problems, and decisions since the last meeting:\n",
            "\n",
            "1. **Backend delay and workaround**\n",
            "   - New problem: The payment gateway integration is more complex than expected, causing a potential delay of about three days.\n",
            "   - Decision: To avoid slipping the overall timeline, Tom will likely work over the weekend to keep the backend on track (tentatively agreed).\n",
            "\n",
            "2. **Front-end status**\n",
            "   - New development: Front-end work is not just on track but slightly ahead, and Maria reports extra bandwidth.\n",
            "\n",
            "3. **Launch and marketing strategy change**\n",
            "   - New proposal/problem: Concern raised that a large social media push on launch day could overwhelm servers and expose initial bugs.\n",
            "   - Decision: Shift from a big public launch to a **soft launch** (invite-only for the first week), followed by the major social media push afterward.\n"
          ]
        }
      ],
      "execution_count": 6,
      "metadata": {
        "id": "1LhMAk4-SggH",
        "outputId": "9d612e83-18dd-4df0-dca4-8fb17ee75291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 2: Conducting the Investigation (The 'How')\n",
        "\n",
        "Now we move from identifying facts to generating insights, the core of the semantic context interpreation journey.\n",
        "\n",
        "1.  **Cell 6: g4 - Identify the Key Threads**\n",
        "\n",
        "      * This is a crucial step. The prompt asks the AI to read between the lines."
      ],
      "metadata": {
        "id": "wh2UWr4ySggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: g4 - Uncovering Implicit Threads\n",
        "prompt_g4 = f\"\"\"\n",
        "Task: Analyze the following meeting content for implicit social dynamics and unstated feelings. Go beyond the literal words.\n",
        "- Did anyone seem hesitant or reluctant despite agreeing to something?\n",
        "- Were there any underlying disagreements or tensions?\n",
        "- What was the overall mood?\n",
        "\n",
        "Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g4 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g4}]\n",
        "    )\n",
        "    implicit_threads = response_g4.choices[0].message.content\n",
        "    print(\"--- IMPLICIT THREADS AND DYNAMICS ---\")\n",
        "    print(implicit_threads)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- IMPLICIT THREADS AND DYNAMICS ---\n",
            "Here’s what seems to be going on beneath the surface:\n",
            "\n",
            "1. **Hesitation / Reluctance**\n",
            "   - **Tom and weekend work:**  \n",
            "     - He says, “I suppose I could work over the weekend… I’d rather not, but I can see the bind we’re in.”  \n",
            "     - The phrasing (“I suppose,” “I’d rather not”) shows clear reluctance. He’s agreeing, but it sounds like a pressured concession rather than a voluntary commitment.\n",
            "     - Sarah’s response, “Appreciate that, Tom. Let’s tentatively agree on that,” treats his offer as the solution without exploring alternatives, suggesting he may feel boxed in.\n",
            "\n",
            "2. **Underlying Disagreements / Tensions**\n",
            "   - **Schedule pressure:**  \n",
            "     - Maria’s reaction (“Three days?... we don’t have that buffer.”) is firm and a bit sharp. There’s a subtle tension between:\n",
            "       - Tom’s need for realistic technical time (integration complexity).\n",
            "       - Maria’s focus on the overall project timeline and launch risk.\n",
            "     - This is not an open conflict, but it hints at friction between engineering realities and project constraints.\n",
            "   - **Risk tolerance around launch and marketing:**\n",
            "     - Tom: “Seems standard” about the big social media push—he doesn’t object, but he also doesn’t show active concern about operational risk.\n",
            "     - Maria: Directly challenges the idea (“I think that’s a mistake”) and proposes a structured alternative (soft launch, invite-only, then big push).\n",
            "     - Sarah quickly endorses Maria’s suggestion as “a very good point” and “much safer,” implicitly siding with Maria’s more cautious approach over the original (and over Tom’s nonchalant acceptance).\n",
            "     - This could leave Tom feeling slightly sidelined or overruled, even if he doesn’t voice it.\n",
            "\n",
            "3. **Role Dynamics**\n",
            "   - **Sarah’s leadership style:**  \n",
            "     - She is decisive and aligns quickly with whoever presents a clearer, risk-aware plan (Maria in both the timeline concern and marketing strategy).\n",
            "     - By moving quickly to “Let’s go with that” and “Okay, great meeting,” she may be closing discussion before emotions or dissent surface.\n",
            "   - **Maria’s position:**  \n",
            "     - Confident, assertive, and solution-oriented. She not only raises issues but offers concrete alternatives.\n",
            "     - Sarah’s strong validation may reinforce Maria’s influence in the group.\n",
            "   - **Tom’s position:**  \n",
            "     - More reactive and defensive. He reports a delay, immediately faces pressure, and ends up offering personal sacrifice (weekend work) to resolve the tension.\n",
            "     - He doesn’t push back on scope, complexity, or the schedule, which suggests either a tendency to avoid conflict or a sense that challenging timing isn’t welcome.\n",
            "\n",
            "4. **Overall Mood**\n",
            "   - **Surface mood:** Professional, efficient, and task-focused. Discussions are calm; no one is overtly hostile.\n",
            "   - **Underneath:**  \n",
            "     - Mild strain around deadlines and workload, especially for Tom.  \n",
            "     - A sense of urgency from Maria and Sarah about the timeline.  \n",
            "     - Subtle imbalance in whose concerns are prioritized (Maria’s risk and timing concerns get strong backing; Tom’s workload concern is acknowledged but effectively overridden by necessity).\n",
            "   - Net effect: The meeting “works” operationally, but Tom may leave feeling pressured and underappreciated, while Maria likely feels heard and supported, and Sarah feels the risks are under better control albeit at a cost to Tom’s weekend.\n"
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "id": "Tkb3x_cnSggI",
        "outputId": "5b0cafbb-4482-42e9-df40-0d4c0b7a7edf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 7: g5 - Perform \"Intellectual Combinations\"**\n",
        "\n",
        "      * Here, we prompt the AI to be creative and solve a problem by synthesizing different ideas from the meeting."
      ],
      "metadata": {
        "id": "yyd_EZ5HSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: g5 - Generating a Novel Solution\n",
        "prompt_g5 = f\"\"\"\n",
        "Context: In the meeting, Maria suggested a 'soft launch' to avoid server strain, and also mentioned her team has 'extra bandwidth'.\n",
        "Tom is facing a 3-day delay on the backend.\n",
        "\n",
        "Task: Propose a novel, actionable idea that uses Maria's team's extra bandwidth to help mitigate Tom's 3-day delay. Combine these two separate pieces of information into a single solution.\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g5 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g5}]\n",
        "    )\n",
        "    novel_solution = response_g5.choices[0].message.content\n",
        "    print(\"--- NOVEL SOLUTION PROPOSED BY AI ---\")\n",
        "    print(novel_solution)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NOVEL SOLUTION PROPOSED BY AI ---\n",
            "Have Maria’s team use their “extra bandwidth” to run and support an extended soft launch that partially bypasses Tom’s 3‑day backend delay.\n",
            "\n",
            "Concrete plan:\n",
            "\n",
            "1. **Define a limited feature set that doesn’t depend on Tom’s delayed pieces**  \n",
            "   - Maria’s team identifies which user flows can run on mock data, feature flags, or temporary workarounds (e.g., cached responses, stubbed APIs, or reduced functionality).  \n",
            "   - Tom flags the endpoints or features that are delayed; everything else is fair game for the soft launch.\n",
            "\n",
            "2. **Maria’s team builds a “soft-launch compatibility layer”**  \n",
            "   Using their extra bandwidth, Maria’s team:  \n",
            "   - Implements temporary mock services / stubs for the delayed backend endpoints.  \n",
            "   - Adds feature flags so the app can switch from mock to real backend with a config change once Tom is done.  \n",
            "   - Adjusts the UI/UX to gracefully degrade where data is incomplete (e.g., “Detailed stats available in 3 days”).\n",
            "\n",
            "3. **Run a controlled soft launch with real users**  \n",
            "   - Release to a small segment so server strain is low, in line with Maria’s original suggestion.  \n",
            "   - Collect metrics, bug reports, and UX feedback on everything that doesn’t depend on Tom’s pending work.\n",
            "\n",
            "4. **Use the 3 days to harden everything else**  \n",
            "   While Tom finishes the backend:  \n",
            "   - Maria’s team fixes front-end issues, improves onboarding, and refines flows based on soft-launch feedback.  \n",
            "   - They also write integration tests that will run automatically once Tom’s endpoints are ready.\n",
            "\n",
            "5. **Flip from soft-launch mode to full mode once Tom is done**  \n",
            "   - When Tom delivers, Maria’s team removes/turns off the mocks and updates feature flags to point to the real backend.  \n",
            "   - Because behavior was already tested with the compatibility layer, the final switch is low risk.\n",
            "\n",
            "This way, Maria’s extra capacity is directly used to turn the “soft launch” idea into a mitigation strategy: you get user validation, bug-finding, and progress toward launch, instead of 3 days of waiting on backend completion.\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "8JN5E5X4SggI",
        "outputId": "85dc62a1-b02d-4747-ecc2-eb17fe04fdd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 3: Determining the Action (The 'What Next')\n",
        "\n",
        "Finally, we turn the analysis into concrete, forward-looking artifacts.\n",
        "\n",
        "1.  **Cell 8: g6 - Define the Output Format (Final Summary)**\n",
        "\n",
        "      * We compile all the key information into a structured, final summary, showing the importance of clear outputs."
      ],
      "metadata": {
        "id": "J0CD-ztrSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: g6 - Creating the Final, Structured Summary\n",
        "prompt_g6 = f\"\"\"\n",
        "Task: Create a final, concise summary of the meeting in a markdown table.\n",
        "Use the following information to construct the table.\n",
        "\n",
        "- New Developments: {new_developments}\n",
        "\n",
        "The table should have three columns: \"Topic\", \"Decision/Outcome\", and \"Owner\".\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g6 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g6}]\n",
        "    )\n",
        "    final_summary_table = response_g6.choices[0].message.content\n",
        "    print(\"--- FINAL MEETING SUMMARY TABLE ---\")\n",
        "    print(final_summary_table)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- FINAL MEETING SUMMARY TABLE ---\n",
            "| Topic                              | Decision/Outcome                                                                                          | Owner  |\n",
            "|------------------------------------|-----------------------------------------------------------------------------------------------------------|--------|\n",
            "| Backend delay & payment gateway    | Payment integration more complex than expected; Tom to likely work over the weekend to avoid timeline slip | Tom    |\n",
            "| Front-end status                   | Front-end is slightly ahead of schedule; Maria has extra bandwidth                                       | Maria  |\n",
            "| Launch & marketing strategy change | Move from big public launch to a one-week invite-only soft launch, then major social media push afterward | Team / Marketing Lead\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "xXRnPWuESggI",
        "outputId": "51a89880-6759-4415-8854-01aff4c1202e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 9: g7 - Generate the Subsequent Task**\n",
        "\n",
        "      * The last step is to use the analysis to perform a real-world action, closing the loop from insight to action."
      ],
      "metadata": {
        "id": "jk1g9wVvSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: g7 - Drafting the Follow-Up Action\n",
        "prompt_g7 = f\"\"\"\n",
        "Task: Based on the following summary table, draft a polite and professional follow-up email to the team (Sarah, Tom, Maria).\n",
        "The email should clearly state the decisions made and the action items for each person.\n",
        "\n",
        "Summary Table:\n",
        "---\n",
        "{final_summary_table}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g7 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g7}]\n",
        "    )\n",
        "    follow_up_email = response_g7.choices[0].message.content\n",
        "    print(\"--- DRAFT FOLLOW-UP EMAIL ---\")\n",
        "    print(follow_up_email)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- DRAFT FOLLOW-UP EMAIL ---\n",
            "Subject: Recap: Decisions & Next Steps from Today’s Meeting\n",
            "\n",
            "Hi Sarah, Tom, and Maria,\n",
            "\n",
            "Following up on our discussion, here is a quick summary of decisions and action items:\n",
            "\n",
            "1. Backend Delay & Payment Gateway  \n",
            "   - Decision: The payment integration is more complex than expected, and we want to avoid slipping the overall timeline.  \n",
            "   - Action (Tom): You’ll review the remaining integration work and, if necessary, work over the weekend to keep us on track. Please let us know by tomorrow if you anticipate needing additional support or any scope adjustments.\n",
            "\n",
            "2. Front-End Status  \n",
            "   - Decision: The front-end is slightly ahead of schedule, giving us some extra capacity.  \n",
            "   - Action (Maria): You’ll continue progressing the front-end and be available to support backend-related UI changes or integration tasks as needed. Please coordinate directly with Tom on priorities.\n",
            "\n",
            "3. Launch & Marketing Strategy  \n",
            "   - Decision: We will shift from a big public launch to:\n",
            "     - A one-week invite-only soft launch, followed by  \n",
            "     - A major social media push afterward.  \n",
            "   - Action (Team / Sarah as Marketing Lead):  \n",
            "     - Sarah: Please outline the soft launch invite criteria, messaging, and timeline, and prepare the social media campaign plan for the post–soft launch push.  \n",
            "     - All: Review Sarah’s plan once shared and provide feedback so we can finalize the launch schedule.\n",
            "\n",
            "If I’ve missed or misrepresented anything, please reply-all with corrections or additions.\n",
            "\n",
            "Thanks everyone,  \n",
            "[Your Name]\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "aQ1PzmTlSggI",
        "outputId": "88155564-19cd-4ac6-a0ae-06301cf47d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook structure doesn't just \"get a summary.\" It takes the user on a journey, showing them *how* to think with the AI as a partner. It perfectly translates the abstract \"Scope, Investigation, Action\" framework into a repeatable, educational, and powerful process."
      ],
      "metadata": {
        "id": "-tDIxvHSSggJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}