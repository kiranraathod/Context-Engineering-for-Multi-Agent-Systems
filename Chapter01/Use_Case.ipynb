{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A Meeting Analysis Use Case\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "The notebook's goal is to demonstrate how to guide an AI through a multi-step analytical process, moving from a raw transcript to actionable insights, thereby training both the user and the AI.\n",
        "\n",
        "This notebook is a step-by-step guide on how you can engineer a structured context . This guide will serve as the architectural blueprint for your code, explaining not just the *what* but the *why* at each stage.\n"
      ],
      "metadata": {
        "id": "FDBDBUKfSggD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Setup and Preliminaries\n",
        "\n",
        "This section handles the basic configuration.\n",
        "\n",
        "1.  **Cell 1: Install Libraries**\n",
        "\n",
        "      * Install the necessary OpenAI library."
      ],
      "metadata": {
        "id": "ivJQbp_mwtDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Installation\n",
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/install.py\", line 362, in run\n",
            "    resolver = self.make_resolver(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/req_command.py\", line 177, in make_resolver\n",
            "    return pip._internal.resolution.resolvelib.resolver.Resolver(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 58, in __init__\n",
            "    self.factory = Factory(\n",
            "                   ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/resolution/resolvelib/factory.py\", line 129, in __init__\n",
            "    for dist in env.iter_installed_distributions(local_only=False)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/metadata/base.py\", line 664, in <genexpr>\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "voHBQKGtSggF",
        "outputId": "642d9d84-69b4-48da-c731-aa02084e8584",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 2: Imports and API Key**\n",
        "\n",
        "      * Import the library and securely prompt for the user's API key. This is better than hard-coding it."
      ],
      "metadata": {
        "id": "jtGvDLpLSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "eNHm5qkhSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  **Cell 3: The Raw Data (The \"Crime Scene\")**\n",
        "\n",
        "      * Define the meeting transcript as a multi-line string. This is our primary data source."
      ],
      "metadata": {
        "id": "Vgp62z7OSggG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: The Full Meeting Transcript\n",
        "meeting_transcript = \"\"\"\n",
        "        Tom: Morning all. Coffee is still kicking in.\n",
        "        Sarah: Morning, Tom. Right, let's jump in. Project Phoenix timeline. Tom, you said the backend components are on track?\n",
        "        Tom: Mostly. We hit a small snag with the payment gateway integration. It's... more complex than the docs suggested. We might need another three days.\n",
        "        Maria: Three days? Tom, that's going to push the final testing phase right up against the launch deadline. We don't have that buffer.\n",
        "        Sarah: I agree with Maria. What's the alternative, Tom?\n",
        "        Tom: I suppose I could work over the weekend to catch up. I'd rather not, but I can see the bind we're in.\n",
        "        Sarah: Appreciate that, Tom. Let's tentatively agree on that. Maria, what about the front-end?\n",
        "        Maria: We're good. In fact, we're a bit ahead. We have some extra bandwidth.\n",
        "        Sarah: Excellent. Okay, one last thing. The marketing team wants to do a big social media push on launch day. Thoughts?\n",
        "        Tom: Seems standard.\n",
        "        Maria: I think that's a mistake. A big push on day one will swamp our servers if there are any initial bugs. We should do a soft launch, invite-only for the first week, and then do the big push. More controlled.\n",
        "        Sarah: That's a very good point, Maria. A much safer strategy. Let's go with that. Okay, great meeting. I'll send out a summary.\n",
        "        Tom: Sounds good. Now, more coffee.\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "UJpZUVXpSggG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 1: Establishing the scope (The 'What')\n",
        "\n",
        "Here, we define the scope of the analysis. Each step's output will inform the next, creating a chain of context.\n",
        "\n",
        "1.  **Cell 4: g2 - Set the Signal-to-Noise Ratio**\n",
        "\n",
        "      * We'll start by cleaning the data. The prompt explicitly tells the AI to separate substantive content from conversational noise."
      ],
      "metadata": {
        "id": "40PTjmLaSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: g2 - Isolating Content from Noise\n",
        "prompt_g2 = f\"\"\"\n",
        "        Analyze the following meeting transcript. Your task is to isolate the substantive content from the conversational noise.\n",
        "        - Substantive content includes: decisions made, project updates, problems raised, and strategic suggestions.\n",
        "        - Noise includes: greetings, pleasantries, and off-topic remarks (like coffee).\n",
        "        Return ONLY the substantive content.\n",
        "\n",
        "        Transcript:\n",
        "        ---\n",
        "        {meeting_transcript}\n",
        "        ---\n",
        "        \"\"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "069eAXt9SggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "try:\n",
        "    client = OpenAI()\n",
        "\n",
        "    response_g2 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt_g2}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    substantive_content = response_g2.choices[0].message.content\n",
        "    print(\"--- SUBSTANTIVE CONTENT ---\")\n",
        "    print(substantive_content)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "ptfVZHlps3r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 5: g3 - Define the Scope of Time (Simulated RAG)**\n",
        "\n",
        "      * We'll simulate a RAG context by providing a \"previous\" summary and asking for what's new. This teaches the user the importance of historical context."
      ],
      "metadata": {
        "id": "XY2ZlN8sSggH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: g3 - Identifying NEW Information (Simulated RAG)\n",
        "previous_summary = \"In our last meeting, we finalized the goals for Project Phoenix and assigned backend work to Tom and front-end to Maria.\"\n",
        "\n",
        "prompt_g3 = f\"\"\"\n",
        "Context: The summary of our last meeting was: \"{previous_summary}\"\n",
        "\n",
        "Task: Analyze the following substantive content from our new meeting. Identify and summarize ONLY the new developments, problems, or decisions that have occurred since the last meeting.\n",
        "\n",
        "New Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Your chat completion request\n",
        "try:\n",
        "    response_g3 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g3}]\n",
        "    )\n",
        "    new_developments = response_g3.choices[0].message.content\n",
        "    print(\"--- NEW DEVELOPMENTS SINCE LAST MEETING ---\")\n",
        "    print(new_developments)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "1LhMAk4-SggH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 2: Conducting the Investigation (The 'How')\n",
        "\n",
        "Now we move from identifying facts to generating insights, the core of the semantic context interpreation journey.\n",
        "\n",
        "1.  **Cell 6: g4 - Identify the Key Threads**\n",
        "\n",
        "      * This is a crucial step. The prompt asks the AI to read between the lines."
      ],
      "metadata": {
        "id": "wh2UWr4ySggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: g4 - Uncovering Implicit Threads\n",
        "prompt_g4 = f\"\"\"\n",
        "Task: Analyze the following meeting content for implicit social dynamics and unstated feelings. Go beyond the literal words.\n",
        "- Did anyone seem hesitant or reluctant despite agreeing to something?\n",
        "- Were there any underlying disagreements or tensions?\n",
        "- What was the overall mood?\n",
        "\n",
        "Meeting Content:\n",
        "---\n",
        "{substantive_content}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g4 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g4}]\n",
        "    )\n",
        "    implicit_threads = response_g4.choices[0].message.content\n",
        "    print(\"--- IMPLICIT THREADS AND DYNAMICS ---\")\n",
        "    print(implicit_threads)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Tkb3x_cnSggI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 7: g5 - Perform \"Intellectual Combinations\"**\n",
        "\n",
        "      * Here, we prompt the AI to be creative and solve a problem by synthesizing different ideas from the meeting."
      ],
      "metadata": {
        "id": "yyd_EZ5HSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: g5 - Generating a Novel Solution\n",
        "prompt_g5 = f\"\"\"\n",
        "Context: In the meeting, Maria suggested a 'soft launch' to avoid server strain, and also mentioned her team has 'extra bandwidth'.\n",
        "Tom is facing a 3-day delay on the backend.\n",
        "\n",
        "Task: Propose a novel, actionable idea that uses Maria's team's extra bandwidth to help mitigate Tom's 3-day delay. Combine these two separate pieces of information into a single solution.\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g5 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g5}]\n",
        "    )\n",
        "    novel_solution = response_g5.choices[0].message.content\n",
        "    print(\"--- NOVEL SOLUTION PROPOSED BY AI ---\")\n",
        "    print(novel_solution)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8JN5E5X4SggI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### Layer 3: Determining the Action (The 'What Next')\n",
        "\n",
        "Finally, we turn the analysis into concrete, forward-looking artifacts.\n",
        "\n",
        "1.  **Cell 8: g6 - Define the Output Format (Final Summary)**\n",
        "\n",
        "      * We compile all the key information into a structured, final summary, showing the importance of clear outputs."
      ],
      "metadata": {
        "id": "J0CD-ztrSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: g6 - Creating the Final, Structured Summary\n",
        "prompt_g6 = f\"\"\"\n",
        "Task: Create a final, concise summary of the meeting in a markdown table.\n",
        "Use the following information to construct the table.\n",
        "\n",
        "- New Developments: {new_developments}\n",
        "\n",
        "The table should have three columns: \"Topic\", \"Decision/Outcome\", and \"Owner\".\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g6 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g6}]\n",
        "    )\n",
        "    final_summary_table = response_g6.choices[0].message.content\n",
        "    print(\"--- FINAL MEETING SUMMARY TABLE ---\")\n",
        "    print(final_summary_table)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "xXRnPWuESggI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  **Cell 9: g7 - Generate the Subsequent Task**\n",
        "\n",
        "      * The last step is to use the analysis to perform a real-world action, closing the loop from insight to action."
      ],
      "metadata": {
        "id": "jk1g9wVvSggI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: g7 - Drafting the Follow-Up Action\n",
        "prompt_g7 = f\"\"\"\n",
        "Task: Based on the following summary table, draft a polite and professional follow-up email to the team (Sarah, Tom, Maria).\n",
        "The email should clearly state the decisions made and the action items for each person.\n",
        "\n",
        "Summary Table:\n",
        "---\n",
        "{final_summary_table}\n",
        "---\n",
        "\"\"\"\n",
        "try:\n",
        "    response_g7 = client.chat.completions.create(\n",
        "        model=\"gpt-5.1\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt_g7}]\n",
        "    )\n",
        "    follow_up_email = response_g7.choices[0].message.content\n",
        "    print(\"--- DRAFT FOLLOW-UP EMAIL ---\")\n",
        "    print(follow_up_email)\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "aQ1PzmTlSggI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook structure doesn't just \"get a summary.\" It takes the user on a journey, showing them *how* to think with the AI as a partner. It perfectly translates the abstract \"Scope, Investigation, Action\" framework into a repeatable, educational, and powerful process."
      ],
      "metadata": {
        "id": "-tDIxvHSSggJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}