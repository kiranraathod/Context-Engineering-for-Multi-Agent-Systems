{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfzZQTDjpfu6"
      },
      "source": [
        "# Context Engine\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "\n",
        "**Building the Context Engine**\n",
        "\n",
        "*From a Team of Agents to an Intelligent System*\n",
        "\n",
        "In the previous chapters, we engineered individual contexts and specialist agents. However, as our system grows, managing these agents in a fixed, linear sequence becomes challenging and rigid. The next evolutionary step is to create a system that can think, plan, and orchestrate these agents dynamically to achieve a high-level goal.\n",
        "\n",
        "This notebook introduces the **Context Engine**, an intelligent controller designed to transform a vague user request into a carefully generated, context-aware output. It acts as an orchestrator, delegating responsibilities to specialized components rather than solving tasks by itself.\n",
        "\n",
        "**Key Innovation: Dynamic, LLM-Powered Planning**\n",
        "\n",
        "The true innovation in this chapter is moving away from hardcoded workflows. We will build a Planner that uses an external LLM to analyze a user's goal. By consulting a registry of available tools, this Planner generates a custom, multi-step JSON plan on the fly. This powerful design separates the \"what to do\" (the goal) from the \"how to do it\" (the plan).\n",
        "\n",
        "In this notebook, you will build:\n",
        "\n",
        "**The Specialist Agents:** The `Librarian`, `Researcher`, and `Writer` from our previous work, who handle style, facts, and content generation.\n",
        "\n",
        "**The Agent Registry:** A \"toolkit\" that describes the capabilities of each agent, making them discoverable to the Planner.\n",
        "\n",
        "The **Engine's \"Brain\"**: The core orchestrator, which includes:\n",
        "The Planner that creates the strategic plan.\n",
        "The Executor that follows the plan and manages Context Chaining, where one agent's output seamlessly becomes the next agent's input.\n",
        "The Tracer that logs the entire process for transparency and debugging.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1bEq01K2Nmz"
      },
      "source": [
        "# 1.Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "outputs": [],
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9fssMtAwGlg",
        "outputId": "480d0136-2ecf-4891-9328-51d9adafdcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gemini client initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the Google GenAI library to interact with the LLM.\n",
        "# We load the API key from a local .env file.\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "try:\n",
        "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise ValueError(\"GOOGLE_API_KEY not found in environment variables. Please check your .env file.\")\n",
        "\n",
        "    # Create client\n",
        "    client = genai.Client(api_key=api_key)\n",
        "    print(\"Gemini client initialized successfully.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"models/gemini-embedding-001\"\n",
        "EMBEDDING_DIM = 3072 # Dimension for text-embedding-004\n",
        "GENERATION_MODEL = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ratho\\Desktop\\data analysis\\clone_github\\Context-Engineering-for-Multi-Agent-Systems\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6V_5MOsBeRl",
        "outputId": "2aa903f0-0139-4719-c52c-0fa40b6d9685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone API key loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the key using the name found in your .env file\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API')\n",
        "if not PINECONE_API_KEY:\n",
        "    print(\"Warning: 'PINECONE_API' not found in environment variables. Please check your .env file.\")\n",
        "else:\n",
        "    print(\"Pinecone API key loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxctIvv62hOm"
      },
      "source": [
        "## 2.Initialize Clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqAbeOskEjP4",
        "outputId": "c7fc1723-abb4-4b90-c35c-f95b5a3cc8ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n",
            "Clearing namespaces for a fresh start...\n"
          ]
        }
      ],
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "    print(\"Clearing namespaces for a fresh start...\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpnNWNgcB_jY"
      },
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwEeJUGhCp0-",
        "outputId": "7fd8e48b-04c4-46ed-fae1-f82cb6af1df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ],
      "source": [
        "#3. Helper Functions (LLM, Embeddings, MCP, Pinecone)\n",
        "# -------------------------------------------------------------------------\n",
        "# Utility functions to standardize interactions.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === LLM Interaction ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm_robust(system_prompt, user_prompt, json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries (Gemini Version).\"\"\"\n",
        "    try:\n",
        "        # 1. Configure for JSON or Text\n",
        "        mime_type = \"application/json\" if json_mode else \"text/plain\"\n",
        "        \n",
        "        # 2. Make the Gemini API call\n",
        "        response = client.models.generate_content(\n",
        "            model=GENERATION_MODEL, # e.g. \"gemini-2.0-flash-exp\"\n",
        "            contents=user_prompt,\n",
        "            config=types.GenerateContentConfig(\n",
        "                system_instruction=system_prompt,\n",
        "                temperature=1.0, # Adjust as needed\n",
        "                response_mime_type=mime_type\n",
        "            )\n",
        "        )\n",
        "        \n",
        "        # 3. Return text (Gemini doesn't use .choices[0].message)\n",
        "        return response.text.strip()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        raise e\n",
        "\n",
        "# === Embeddings ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.models.embed_content(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        contents=text\n",
        "    )\n",
        "    # Gemini 2.0 SDK returns a list of embeddings. We take the first one.\n",
        "    return response.embeddings[0].values\n",
        "\n",
        "# === Model Context Protocol (MCP) ===\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message.\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"2.0 (Context Engine)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content, # The actual payload/context\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "# === Pinecone Interaction ===\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        raise e\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFStlpjzCwNR",
        "outputId": "17ad2039-6fa1-4577-c220-0cd58dac30af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Specialist Agents defined.\n"
          ]
        }
      ],
      "source": [
        "#@title 4.The Specialist Agents (The Handlers)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the specialist agents. These are largely reused from Chapter 3,\n",
        "# but enhanced to handle more flexible inputs required for dynamic planning.\n",
        "# Agents return the raw data (string) as the MCP 'content' for simplicity.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    requested_intent = mcp_message['content'].get('intent_query')\n",
        "\n",
        "    if not requested_intent:\n",
        "        raise ValueError(\"Librarian requires 'intent_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        # The output content IS the blueprint itself (as a string)\n",
        "        content = blueprint_json\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = json.dumps({\"instruction\": \"Generate the content neutrally.\"})\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    topic = mcp_message['content'].get('topic_query')\n",
        "\n",
        "    if not topic:\n",
        "        raise ValueError(\"Researcher requires 'topic_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        # Return a string indicating no data found\n",
        "        return create_mcp_message(\"Researcher\", \"No data found on the topic.\")\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    # Use a low temperature for factual synthesis\n",
        "    findings = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the findings (as a string)\n",
        "    return create_mcp_message(\"Researcher\", findings)\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    Crucially enhanced to handle either raw facts OR previous content for rewriting tasks.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to source material...\")\n",
        "\n",
        "    # Extract inputs.\n",
        "    blueprint_json_string = mcp_message['content'].get('blueprint')\n",
        "    # Check for 'facts' first, then 'previous_content'\n",
        "    facts = mcp_message['content'].get('facts')\n",
        "    previous_content = mcp_message['content'].get('previous_content')\n",
        "\n",
        "    if not blueprint_json_string:\n",
        "         raise ValueError(\"Writer requires 'blueprint' in the input content.\")\n",
        "\n",
        "    # Determine the source material and label for the prompt\n",
        "    if facts: \n",
        "        source_material = facts\n",
        "        source_label = \"RESEARCH FINDINGS\"\n",
        "    elif previous_content:\n",
        "        source_material = previous_content\n",
        "        source_label = \"PREVIOUS CONTENT (For Rewriting)\"\n",
        "    else:\n",
        "        raise ValueError(\"Writer requires either 'facts' or 'previous_content'.\")\n",
        "\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided SOURCE MATERIAL.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the source material defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- SOURCE MATERIAL ({source_label}) ---\n",
        "    {source_material}\n",
        "    --- END SOURCE MATERIAL ---\n",
        "\n",
        "    Generate the content now, following the blueprint precisely.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the generated text (as a string)\n",
        "    return create_mcp_message(\"Writer\", final_output)\n",
        "\n",
        "print(\"Specialist Agents defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HzVg_IMC_yw",
        "outputId": "609a79de-b695-45eb-8be6-610b403b97ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent Registry initialized.\n"
          ]
        }
      ],
      "source": [
        "#@title 5.The Agent Registry (The Toolkit)\n",
        "# -------------------------------------------------------------------------\n",
        "# We formalize the \"Handler Registry\" into an AgentRegistry.\n",
        "# This catalogs agents and describes their capabilities to the Planner.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "class AgentRegistry:\n",
        "    def __init__(self):\n",
        "        # Mapping of agent names to their corresponding functions\n",
        "        self.registry = {\n",
        "            \"Librarian\": agent_context_librarian,\n",
        "            \"Researcher\": agent_researcher,\n",
        "            \"Writer\": agent_writer,\n",
        "        }\n",
        "\n",
        "    def get_handler(self, agent_name):\n",
        "        \"\"\"Retrieves the function associated with an agent name.\"\"\"\n",
        "        handler = self.registry.get(agent_name)\n",
        "        if not handler:\n",
        "            raise ValueError(f\"Agent '{agent_name}' not found in registry.\")\n",
        "        return handler\n",
        "\n",
        "    def get_capabilities_description(self):\n",
        "        \"\"\"\n",
        "        Returns a structured description of the agents for the Planner LLM.\n",
        "        This is crucial for the Planner to understand how to use the agents.\n",
        "        \"\"\"\n",
        "        return \"\"\"\n",
        "        Available Agents and their required inputs:\n",
        "\n",
        "        1. AGENT: Librarian\n",
        "           ROLE: Retrieves Semantic Blueprints (style/structure instructions).\n",
        "           INPUTS:\n",
        "             - \"intent_query\": (String) A descriptive phrase of the desired style or format.\n",
        "           OUTPUT: The blueprint structure (JSON string).\n",
        "\n",
        "        2. AGENT: Researcher\n",
        "           ROLE: Retrieves and synthesizes factual information on a topic.\n",
        "           INPUTS:\n",
        "             - \"topic_query\": (String) The subject matter to research.\n",
        "           OUTPUT: Synthesized facts (String).\n",
        "\n",
        "        3. AGENT: Writer\n",
        "           ROLE: Generates or rewrites content by applying a Blueprint to source material.\n",
        "           INPUTS:\n",
        "             - \"blueprint\": (String/Reference) The style instructions (usually from Librarian).\n",
        "             - \"facts\": (String/Reference) Factual information (usually from Researcher). Use this for new content generation.\n",
        "             - \"previous_content\": (String/Reference) Existing text (usually from a prior Writer step). Use this for rewriting/adapting content.\n",
        "           OUTPUT: The final generated text (String).\n",
        "        \"\"\"\n",
        "\n",
        "# Initialize the global toolkit\n",
        "AGENT_TOOLKIT = AgentRegistry()\n",
        "print(\"Agent Registry initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SpHqskOtDHlZ"
      },
      "outputs": [],
      "source": [
        "#@title 6.The Context Engine (Planner, Executor, Tracer)\n",
        "# -------------------------------------------------------------------------\n",
        "# This is the core innovation of Chapter 4. It replaces the linear\n",
        "# Orchestrator with a dynamic, LLM-driven planning and execution system.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 6.1. The Tracer (Debugging Implementation) ===\n",
        "class ExecutionTrace:\n",
        "    \"\"\"Logs the entire execution flow for debugging and analysis.\"\"\"\n",
        "    def __init__(self, goal):\n",
        "        self.goal = goal\n",
        "        self.plan = None\n",
        "        self.steps = []\n",
        "        self.status = \"Initialized\"\n",
        "        self.final_output = None\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_plan(self, plan):\n",
        "        self.plan = plan\n",
        "\n",
        "    def log_step(self, step_num, agent, planned_input, mcp_output, resolved_input):\n",
        "        \"\"\"Logs the details of a single execution step.\"\"\"\n",
        "        self.steps.append({\n",
        "            \"step\": step_num,\n",
        "            \"agent\": agent,\n",
        "             # The raw input definitions from the plan (including $$REFS$$)\n",
        "            \"planned_input\": planned_input,\n",
        "            # Crucial for debugging: What exact context did the agent receive?\n",
        "            \"resolved_context\": resolved_input,\n",
        "            \"output\": mcp_output['content']\n",
        "        })\n",
        "\n",
        "    def finalize(self, status, final_output=None):\n",
        "        self.status = status\n",
        "        self.final_output = final_output\n",
        "        self.duration = time.time() - self.start_time\n",
        "\n",
        "    def display_trace(self):\n",
        "        \"\"\"Displays the trace in a readable format.\"\"\"\n",
        "        display(Markdown(f\"### Execution Trace\\n**Goal:** {self.goal}\\n**Status:** {self.status} (Duration: {self.duration:.2f}s)\"))\n",
        "        if self.plan:\n",
        "            # Display the raw plan JSON\n",
        "            display(Markdown(f\"#### Plan:\\n```json\\n{json.dumps(self.plan, indent=2)}\\n```\"))\n",
        "\n",
        "        display(Markdown(\"#### Execution Steps:\"))\n",
        "        for step in self.steps:\n",
        "            print(f\"--- Step {step['step']}: {step['agent']} ---\")\n",
        "            print(\"  [Planned Input]:\", step['planned_input'])\n",
        "            # print(\"  [Resolved Context]:\", textwrap.shorten(str(step['resolved_context']), width=150))\n",
        "            print(\"  [Output Snippet]:\", textwrap.shorten(str(step['output']), width=150))\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# === 6.2. The Planner (Strategic Analysis) ===\n",
        "def planner(goal, capabilities):\n",
        "    \"\"\"\n",
        "    Analyzes the goal and generates a structured Execution Plan using the LLM.\n",
        "    \"\"\"\n",
        "    print(\"[Engine: Planner] Analyzing goal and generating execution plan...\")\n",
        "    system_prompt = f\"\"\"\n",
        "    You are the strategic core of the Context Engine. Analyze the user's high-level goal and create a structured Execution Plan using the available agents.\n",
        "\n",
        "    --- AVAILABLE CAPABILITIES ---\n",
        "    {capabilities}\n",
        "    --- END CAPABILITIES ---\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. The plan MUST be a JSON list of objects, where each object is a \"step\".\n",
        "    2. You MUST use Context Chaining. If a step requires input from a previous step, reference it using the syntax $$STEP_X_OUTPUT$$.\n",
        "    3. Be strategic. Break down complex goals (like sequential rewriting) into distinct steps. Use the correct input keys ('facts' vs 'previous_content') for the Writer agent.\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a suspenseful story about Apollo 11.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"suspenseful narrative blueprint\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Apollo 11 landing details\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a technical report on Juno, then rewrite it casually.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"technical report structure\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Juno mission technology\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}},\n",
        "        {{\"step\": 4, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"casual summary style\"}}}},\n",
        "        {{\"step\": 5, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_4_OUTPUT$$\", \"previous_content\": \"$$STEP_3_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    Respond ONLY with the JSON list.\n",
        "    \"\"\"\n",
        "    # Call LLM in JSON mode for reliability\n",
        "    plan_json = \"\"\n",
        "    try:\n",
        "        plan_json = call_llm_robust(system_prompt, goal, json_mode=True)\n",
        "        plan = json.loads(plan_json)\n",
        "\n",
        "        if not isinstance(plan, list):\n",
        "             # Handle cases where the LLM wraps the list in a dictionary\n",
        "             if isinstance(plan, dict):\n",
        "                 if \"plan\" in plan and isinstance(plan[\"plan\"], list):\n",
        "                     plan = plan[\"plan\"]\n",
        "                 elif \"steps\" in plan and isinstance(plan[\"steps\"], list): # <--- ADD THIS CHECK\n",
        "                     plan = plan[\"steps\"]\n",
        "                 else:\n",
        "                    raise ValueError(\"Planner returned a dict, but missing 'plan' or 'steps' key.\")\n",
        "             else:\n",
        "                raise ValueError(\"Planner did not return a valid JSON list structure.\")\n",
        "\n",
        "        print(\"[Engine: Planner] Plan generated successfully.\")\n",
        "        return plan\n",
        "    except Exception as e:\n",
        "        print(f\"[Engine: Planner] Failed to generate a valid plan. Error: {e}. Raw LLM Output: {plan_json}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# === 6.3. The Executor (Context Assembly and Execution) ===\n",
        "\n",
        "def resolve_dependencies(input_params, state):\n",
        "    \"\"\"\n",
        "    Helper function to replace $$REF$$ placeholders with actual data from the execution state.\n",
        "    This implements Context Chaining.\n",
        "    \"\"\"\n",
        "    # Use copy.deepcopy to ensure the original plan structure is not modified\n",
        "    resolved_input = copy.deepcopy(input_params)\n",
        "\n",
        "    # Recursive function to handle potential nested structures\n",
        "    def resolve(value):\n",
        "        if isinstance(value, str) and value.startswith(\"$$\") and value.endswith(\"$$\"):\n",
        "            ref_key = value[2:-2]\n",
        "            if ref_key in state:\n",
        "                # Retrieve the actual data (string) from the previous step's output\n",
        "                print(f\"[Engine: Executor] Resolved dependency {ref_key}.\")\n",
        "                return state[ref_key]\n",
        "            else:\n",
        "                raise ValueError(f\"Dependency Error: Reference {ref_key} not found in execution state.\")\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: resolve(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [resolve(v) for v in value]\n",
        "        return value\n",
        "\n",
        "    return resolve(resolved_input)\n",
        "\n",
        "\n",
        "def context_engine(goal):\n",
        "    \"\"\"\n",
        "    The main entry point for the Context Engine. Manages Planning and Execution.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== [Context Engine] Starting New Task ===\\nGoal: {goal}\\n\")\n",
        "    trace = ExecutionTrace(goal)\n",
        "    registry = AGENT_TOOLKIT\n",
        "\n",
        "    # Phase 1: Plan\n",
        "    try:\n",
        "        capabilities = registry.get_capabilities_description()\n",
        "        plan = planner(goal, capabilities)\n",
        "        trace.log_plan(plan)\n",
        "    except Exception as e:\n",
        "        trace.finalize(\"Failed during Planning\")\n",
        "        # Return the trace even in failure for debugging\n",
        "        return None, trace\n",
        "\n",
        "    # Phase 2: Execute\n",
        "    # State stores the raw outputs (strings) of each step: { \"STEP_X_OUTPUT\": data_string }\n",
        "    state = {}\n",
        "\n",
        "    for step in plan:\n",
        "        step_num = step.get(\"step\")\n",
        "        agent_name = step.get(\"agent\")\n",
        "        planned_input = step.get(\"input\")\n",
        "\n",
        "        print(f\"\\n[Engine: Executor] Starting Step {step_num}: {agent_name}\")\n",
        "\n",
        "        try:\n",
        "            handler = registry.get_handler(agent_name)\n",
        "\n",
        "            # Context Assembly: Resolve dependencies\n",
        "            resolved_input = resolve_dependencies(planned_input, state)\n",
        "\n",
        "            # Execute Agent via MCP\n",
        "            # Create an MCP message with the RESOLVED input for the agent\n",
        "            mcp_resolved_input = create_mcp_message(\"Engine\", resolved_input)\n",
        "            mcp_output = handler(mcp_resolved_input)\n",
        "\n",
        "            # Update State and Log Trace\n",
        "            output_data = mcp_output[\"content\"]\n",
        "\n",
        "            # Store the output data (the context itself)\n",
        "            state[f\"STEP_{step_num}_OUTPUT\"] = output_data\n",
        "            trace.log_step(step_num, agent_name, planned_input, mcp_output, resolved_input)\n",
        "            print(f\"[Engine: Executor] Step {step_num} completed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Execution failed at step {step_num} ({agent_name}): {e}\"\n",
        "            print(f\"[Engine: Executor] ERROR: {error_message}\")\n",
        "            trace.finalize(f\"Failed at Step {step_num}\")\n",
        "            # Return the trace for debugging the failure\n",
        "            return None, trace\n",
        "\n",
        "    # Finalization\n",
        "    final_output = state.get(f\"STEP_{len(plan)}_OUTPUT\")\n",
        "    trace.finalize(\"Success\", final_output)\n",
        "    print(\"\\n=== [Context Engine] Task Complete ===\")\n",
        "\n",
        "    # Return the output of the final step AND the trace\n",
        "    return final_output, trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WB7udFkcDQBS",
        "outputId": "2481d262-fe77-426d-948e-57b04b073c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\n",
            "\n",
            "\n",
            "=== [Context Engine] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Engine: Planner] Analyzing goal and generating execution plan...\n",
            "[Engine: Planner] Plan generated successfully.\n",
            "\n",
            "[Engine: Executor] Starting Step 1: Librarian\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.81)\n",
            "[Engine: Executor] Step 1 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 2: Researcher\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "[Engine: Executor] Step 2 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 3: Writer\n",
            "[Engine: Executor] Resolved dependency STEP_1_OUTPUT.\n",
            "[Engine: Executor] Resolved dependency STEP_2_OUTPUT.\n",
            "\n",
            "[Writer] Activated. Applying blueprint to source material...\n",
            "[Engine: Executor] Step 3 completed.\n",
            "\n",
            "=== [Context Engine] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "The Eagle plunged. Down it went. Something was wrong. The path shifted. Danger ahead. Armstrong saw it. He knew.\n",
              "\n",
              "The readouts flashed. Warnings. The machine was lost. Its course, uncertain. A silent alarm screamed in the quiet capsule.\n",
              "\n",
              "Then, a worse truth. The fuel gauge dropped. Low. So low. Every second bled precious reserves. No time for error. No room to breathe.\n",
              "\n",
              "Armstrong reached. His hand moved. He took control. Manual override. The fate of the mission. In his hands."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title 7.Execution (Standard Goal)\n",
        "# -------------------------------------------------------------------------\n",
        "# Demonstrate the engine with a standard goal similar to Chapter 3,\n",
        "# showing how the Planner dynamically constructs the workflow.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\\n\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "\n",
        "# Run the Context Engine\n",
        "# Ensure the Pinecone index is populated (from Ch3 notebook) for this to work.\n",
        "result_1, trace_1 = context_engine(goal_1)\n",
        "\n",
        "if result_1:\n",
        "    print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "    display(Markdown(result_1))\n",
        "    print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "    # Optional: Display the trace to see the engine's process\n",
        "    # trace_1.display_trace()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOxUVEPjxJwsJ4BHABdpS0P",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
