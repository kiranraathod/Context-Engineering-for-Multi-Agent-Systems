{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOxUVEPjxJwsJ4BHABdpS0P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context Engine\n",
        "\n",
        "Copyright 2025, Denis Rothman\n",
        "\n",
        "\n",
        "**Building the Context Engine**\n",
        "\n",
        "*From a Team of Agents to an Intelligent System*\n",
        "\n",
        "In the previous chapters, we engineered individual contexts and specialist agents. However, as our system grows, managing these agents in a fixed, linear sequence becomes challenging and rigid. The next evolutionary step is to create a system that can think, plan, and orchestrate these agents dynamically to achieve a high-level goal.\n",
        "\n",
        "This notebook introduces the **Context Engine**, an intelligent controller designed to transform a vague user request into a carefully generated, context-aware output. It acts as an orchestrator, delegating responsibilities to specialized components rather than solving tasks by itself.\n",
        "\n",
        "**Key Innovation: Dynamic, LLM-Powered Planning**\n",
        "\n",
        "The true innovation in this chapter is moving away from hardcoded workflows. We will build a Planner that uses an external LLM to analyze a user's goal. By consulting a registry of available tools, this Planner generates a custom, multi-step JSON plan on the fly. This powerful design separates the \"what to do\" (the goal) from the \"how to do it\" (the plan).\n",
        "\n",
        "In this notebook, you will build:\n",
        "\n",
        "**The Specialist Agents:** The `Librarian`, `Researcher`, and `Writer` from our previous work, who handle style, facts, and content generation.\n",
        "\n",
        "**The Agent Registry:** A \"toolkit\" that describes the capabilities of each agent, making them discoverable to the Planner.\n",
        "\n",
        "The **Engine's \"Brain\"**: The core orchestrator, which includes:\n",
        "The Planner that creates the strategic plan.\n",
        "The Executor that follows the plan and manages Context Chaining, where one agent's output seamlessly becomes the next agent's input.\n",
        "The Tracer that logs the entire process for transparency and debugging.\n",
        "\n"
      ],
      "metadata": {
        "id": "hfzZQTDjpfu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Installation and Setup"
      ],
      "metadata": {
        "id": "-1bEq01K2Nmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.Installation and Setup\n",
        "# -------------------------------------------------------------------------\n",
        "# We install specific versions for stability and reproducibility.\n",
        "# We include tiktoken for token-based chunking and tenacity for robust API calls."
      ],
      "metadata": {
        "id": "_MlRuXOwA7Ai"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm==4.67.1 --upgrade\n",
        "!pip install openai==2.8.1\n",
        "!pip install pinecone==7.0.0 tqdm==4.67.1 tenacity==8.3.0"
      ],
      "metadata": {
        "id": "NlXCn7y6CQ3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea5b745-c99b-4b6b-8078-28eed719c761"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: openai==2.8.1 in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==2.8.1) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==2.8.1) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==2.8.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==2.8.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==2.8.1) (0.4.2)\n",
            "Requirement already satisfied: pinecone==7.0.0 in /usr/local/lib/python3.12/dist-packages (7.0.0)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: tenacity==8.3.0 in /usr/local/lib/python3.12/dist-packages (8.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2025.10.5)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone==7.0.0) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone==7.0.0) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and API Key Setup\n",
        "# We will use the OpenAI library to interact with the LLM and Google Colab's\n",
        "# secret manager to securely access your API key.\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the API key from Colab secrets, set the env var, then init the client\n",
        "try:\n",
        "    api_key = userdata.get(\"API_KEY\")\n",
        "    if not api_key:\n",
        "        raise userdata.SecretNotFoundError(\"API_KEY not found.\")\n",
        "\n",
        "    # Set environment variable for downstream tools/libraries\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "    # Create client (will read from OPENAI_API_KEY)\n",
        "    client = OpenAI()\n",
        "    print(\"OpenAI API key loaded and environment variable set successfully.\")\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print('Secret \"API_KEY\" not found.')\n",
        "    print('Please add your OpenAI API key to the Colab Secrets Manager.')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the API key: {e}\")\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
        "EMBEDDING_DIM = 1536 # Dimension for text-embedding-3-small\n",
        "GENERATION_MODEL = \"gpt-5.1\""
      ],
      "metadata": {
        "id": "R9fssMtAwGlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480d0136-2ecf-4891-9328-51d9adafdcc9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key loaded and environment variable set successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for this notebook\n",
        "import json\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import tiktoken\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "# general imports required in the notebooks of this book\n",
        "import re\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "import copy"
      ],
      "metadata": {
        "id": "ptErFjUn54u0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Standard way to access secrets securely in Google Colab\n",
        "    from google.colab import userdata\n",
        "    PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        raise ValueError(\"API Keys not found in Colab secrets.\")\n",
        "    print(\"API Keys loaded successfully.\")\n",
        "except ImportError:\n",
        "    # Fallback for non-Colab environments (e.g., local Jupyter)\n",
        "    PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "    if not PINECONE_API_KEY:\n",
        "        print(\"Warning: API Keys not found. Ensure environment variables are set.\")"
      ],
      "metadata": {
        "id": "d6V_5MOsBeRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa903f0-0139-4719-c52c-0fa40b6d9685"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Initialize Clients"
      ],
      "metadata": {
        "id": "dxctIvv62hOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.Initialize Clients\n",
        "# --- Initialize Clients (assuming this is already done) ---\n",
        "\n",
        "# --- Initialize Pinecone Client ---\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# --- Define Index and Namespaces (assuming this is already done) ---\n",
        "INDEX_NAME = 'genai-mas-mcp-ch3'\n",
        "NAMESPACE_KNOWLEDGE = \"KnowledgeStore\"\n",
        "NAMESPACE_CONTEXT = \"ContextLibrary\"\n",
        "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "# Check if index exists\n",
        "if INDEX_NAME not in pc.list_indexes().names():\n",
        "    print(f\"Index '{INDEX_NAME}' not found. Creating new serverless index...\")\n",
        "    pc.create_index(\n",
        "        name=INDEX_NAME,\n",
        "        dimension=EMBEDDING_DIM, # Make sure EMBEDDING_DIM is defined\n",
        "        metric='cosine',\n",
        "        spec=spec\n",
        "    )\n",
        "    # Wait for index to be ready\n",
        "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
        "        print(\"Waiting for index to be ready...\")\n",
        "        time.sleep(1)\n",
        "    print(\"Index created successfully. It is new and empty.\")\n",
        "else:\n",
        "    # This block runs ONLY if the index already existed.\n",
        "    print(f\"Index '{INDEX_NAME}' already exists.\")\n",
        "    print(\"Clearing namespaces for a fresh start...\")\n",
        "\n",
        "    # Connect to the index to perform operations\n",
        "    index = pc.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "yqAbeOskEjP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7fc1723-abb4-4b90-c35c-f95b5a3cc8ce"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'genai-mas-mcp-ch3' already exists.\n",
            "Clearing namespaces for a fresh start...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Helper Functions (LLM, Embeddings, and MCP)"
      ],
      "metadata": {
        "id": "YpnNWNgcB_jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Helper Functions (LLM, Embeddings, MCP, Pinecone)\n",
        "# -------------------------------------------------------------------------\n",
        "# Utility functions to standardize interactions.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === LLM Interaction ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def call_llm_robust(system_prompt, user_prompt,json_mode=False):\n",
        "    \"\"\"A centralized function to handle all LLM interactions with retries.\"\"\"\n",
        "    try:\n",
        "        response_format = {\"type\": \"json_object\"} if json_mode else {\"type\": \"text\"}\n",
        "        response = client.chat.completions.create(\n",
        "            model=GENERATION_MODEL,\n",
        "            response_format=response_format,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling LLM: {e}\")\n",
        "        # Raise the exception so the caller can handle it or the engine can stop\n",
        "        raise e\n",
        "\n",
        "# === Embeddings ===\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_embedding(text):\n",
        "    \"\"\"Generates embeddings for a single text query with retries.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# === Model Context Protocol (MCP) ===\n",
        "def create_mcp_message(sender, content, metadata=None):\n",
        "    \"\"\"Creates a standardized MCP message.\"\"\"\n",
        "    return {\n",
        "        \"protocol_version\": \"2.0 (Context Engine)\",\n",
        "        \"sender\": sender,\n",
        "        \"content\": content, # The actual payload/context\n",
        "        \"metadata\": metadata or {}\n",
        "    }\n",
        "\n",
        "# === Pinecone Interaction ===\n",
        "def query_pinecone(query_text, namespace, top_k=1):\n",
        "    \"\"\"Embeds the query text and searches the specified Pinecone namespace.\"\"\"\n",
        "    try:\n",
        "        query_embedding = get_embedding(query_text)\n",
        "        response = index.query(\n",
        "            vector=query_embedding,\n",
        "            namespace=namespace,\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "        return response['matches']\n",
        "    except Exception as e:\n",
        "        print(f\"Error querying Pinecone (Namespace: {namespace}): {e}\")\n",
        "        raise e\n",
        "\n",
        "print(\"Helper functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwEeJUGhCp0-",
        "outputId": "7fd8e48b-04c4-46ed-fae1-f82cb6af1df2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 4.The Specialist Agents (The Handlers)\n",
        "# -------------------------------------------------------------------------\n",
        "# We define the specialist agents. These are largely reused from Chapter 3,\n",
        "# but enhanced to handle more flexible inputs required for dynamic planning.\n",
        "# Agents return the raw data (string) as the MCP 'content' for simplicity.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 4.1. Context Librarian Agent (Procedural RAG) ===\n",
        "def agent_context_librarian(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves the appropriate Semantic Blueprint from the Context Library.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Librarian] Activated. Analyzing intent...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    requested_intent = mcp_message['content'].get('intent_query')\n",
        "\n",
        "    if not requested_intent:\n",
        "        raise ValueError(\"Librarian requires 'intent_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Context Namespace\n",
        "    results = query_pinecone(requested_intent, NAMESPACE_CONTEXT, top_k=1)\n",
        "\n",
        "    if results:\n",
        "        match = results[0]\n",
        "        print(f\"[Librarian] Found blueprint '{match['id']}' (Score: {match['score']:.2f})\")\n",
        "        # Retrieve the blueprint JSON string stored in metadata\n",
        "        blueprint_json = match['metadata']['blueprint_json']\n",
        "        # The output content IS the blueprint itself (as a string)\n",
        "        content = blueprint_json\n",
        "    else:\n",
        "        print(\"[Librarian] No specific blueprint found. Returning default.\")\n",
        "        # Fallback default\n",
        "        content = json.dumps({\"instruction\": \"Generate the content neutrally.\"})\n",
        "\n",
        "    return create_mcp_message(\"Librarian\", content)\n",
        "\n",
        "# === 4.2. Researcher Agent (Factual RAG) ===\n",
        "def agent_researcher(mcp_message):\n",
        "    \"\"\"\n",
        "    Retrieves and synthesizes factual information from the Knowledge Base.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Researcher] Activated. Investigating topic...\")\n",
        "    # Extract the specific input required by this agent\n",
        "    topic = mcp_message['content'].get('topic_query')\n",
        "\n",
        "    if not topic:\n",
        "        raise ValueError(\"Researcher requires 'topic_query' in the input content.\")\n",
        "\n",
        "    # Query Pinecone Knowledge Namespace\n",
        "    results = query_pinecone(topic, NAMESPACE_KNOWLEDGE, top_k=3)\n",
        "\n",
        "    if not results:\n",
        "        print(\"[Researcher] No relevant information found.\")\n",
        "        # Return a string indicating no data found\n",
        "        return create_mcp_message(\"Researcher\", \"No data found on the topic.\")\n",
        "\n",
        "    # Synthesize the findings (Retrieve-and-Synthesize)\n",
        "    print(f\"[Researcher] Found {len(results)} relevant chunks. Synthesizing...\")\n",
        "    source_texts = [match['metadata']['text'] for match in results]\n",
        "\n",
        "    system_prompt = \"\"\"You are an expert research synthesis AI.\n",
        "    Synthesize the provided source texts into a concise, bullet-pointed summary relevant to the user's topic. Focus strictly on the facts provided in the sources. Do not add outside information.\"\"\"\n",
        "\n",
        "    user_prompt = f\"Topic: {topic}\\n\\nSources:\\n\" + \"\\n\\n---\\n\\n\".join(source_texts)\n",
        "\n",
        "    # Use a low temperature for factual synthesis\n",
        "    findings = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the findings (as a string)\n",
        "    return create_mcp_message(\"Researcher\", findings)\n",
        "\n",
        "# === 4.3. Writer Agent (Generation) ===\n",
        "def agent_writer(mcp_message):\n",
        "    \"\"\"\n",
        "    Combines the factual research with the semantic blueprint to generate the final output.\n",
        "    Crucially enhanced to handle either raw facts OR previous content for rewriting tasks.\n",
        "    \"\"\"\n",
        "    print(\"\\n[Writer] Activated. Applying blueprint to source material...\")\n",
        "\n",
        "    # Extract inputs.\n",
        "    blueprint_json_string = mcp_message['content'].get('blueprint')\n",
        "    # Check for 'facts' first, then 'previous_content'\n",
        "    facts = mcp_message['content'].get('facts')\n",
        "    previous_content = mcp_message['content'].get('previous_content')\n",
        "\n",
        "    if not blueprint_json_string:\n",
        "         raise ValueError(\"Writer requires 'blueprint' in the input content.\")\n",
        "\n",
        "    # Determine the source material and label for the prompt\n",
        "    if facts:\n",
        "        source_material = facts\n",
        "        source_label = \"RESEARCH FINDINGS\"\n",
        "    elif previous_content:\n",
        "        source_material = previous_content\n",
        "        source_label = \"PREVIOUS CONTENT (For Rewriting)\"\n",
        "    else:\n",
        "        raise ValueError(\"Writer requires either 'facts' or 'previous_content'.\")\n",
        "\n",
        "\n",
        "    # The Writer's System Prompt incorporates the dynamically retrieved blueprint\n",
        "    system_prompt = f\"\"\"You are an expert content generation AI.\n",
        "    Your task is to generate content based on the provided SOURCE MATERIAL.\n",
        "    Crucially, you MUST structure, style, and constrain your output according to the rules defined in the SEMANTIC BLUEPRINT provided below.\n",
        "\n",
        "    --- SEMANTIC BLUEPRINT (JSON) ---\n",
        "    {blueprint_json_string}\n",
        "    --- END SEMANTIC BLUEPRINT ---\n",
        "\n",
        "    Adhere strictly to the blueprint's instructions, style guides, and goals. The blueprint defines HOW you write; the source material defines WHAT you write about.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    --- SOURCE MATERIAL ({source_label}) ---\n",
        "    {source_material}\n",
        "    --- END SOURCE MATERIAL ---\n",
        "\n",
        "    Generate the content now, following the blueprint precisely.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the final content (slightly higher temperature for potential creativity)\n",
        "    final_output = call_llm_robust(system_prompt, user_prompt)\n",
        "\n",
        "    # The output content IS the generated text (as a string)\n",
        "    return create_mcp_message(\"Writer\", final_output)\n",
        "\n",
        "print(\"Specialist Agents defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFStlpjzCwNR",
        "outputId": "17ad2039-6fa1-4577-c220-0cd58dac30af"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specialist Agents defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5.The Agent Registry (The Toolkit)\n",
        "# -------------------------------------------------------------------------\n",
        "# We formalize the \"Handler Registry\" into an AgentRegistry.\n",
        "# This catalogs agents and describes their capabilities to the Planner.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "class AgentRegistry:\n",
        "    def __init__(self):\n",
        "        # Mapping of agent names to their corresponding functions\n",
        "        self.registry = {\n",
        "            \"Librarian\": agent_context_librarian,\n",
        "            \"Researcher\": agent_researcher,\n",
        "            \"Writer\": agent_writer,\n",
        "        }\n",
        "\n",
        "    def get_handler(self, agent_name):\n",
        "        \"\"\"Retrieves the function associated with an agent name.\"\"\"\n",
        "        handler = self.registry.get(agent_name)\n",
        "        if not handler:\n",
        "            raise ValueError(f\"Agent '{agent_name}' not found in registry.\")\n",
        "        return handler\n",
        "\n",
        "    def get_capabilities_description(self):\n",
        "        \"\"\"\n",
        "        Returns a structured description of the agents for the Planner LLM.\n",
        "        This is crucial for the Planner to understand how to use the agents.\n",
        "        \"\"\"\n",
        "        return \"\"\"\n",
        "        Available Agents and their required inputs:\n",
        "\n",
        "        1. AGENT: Librarian\n",
        "           ROLE: Retrieves Semantic Blueprints (style/structure instructions).\n",
        "           INPUTS:\n",
        "             - \"intent_query\": (String) A descriptive phrase of the desired style or format.\n",
        "           OUTPUT: The blueprint structure (JSON string).\n",
        "\n",
        "        2. AGENT: Researcher\n",
        "           ROLE: Retrieves and synthesizes factual information on a topic.\n",
        "           INPUTS:\n",
        "             - \"topic_query\": (String) The subject matter to research.\n",
        "           OUTPUT: Synthesized facts (String).\n",
        "\n",
        "        3. AGENT: Writer\n",
        "           ROLE: Generates or rewrites content by applying a Blueprint to source material.\n",
        "           INPUTS:\n",
        "             - \"blueprint\": (String/Reference) The style instructions (usually from Librarian).\n",
        "             - \"facts\": (String/Reference) Factual information (usually from Researcher). Use this for new content generation.\n",
        "             - \"previous_content\": (String/Reference) Existing text (usually from a prior Writer step). Use this for rewriting/adapting content.\n",
        "           OUTPUT: The final generated text (String).\n",
        "        \"\"\"\n",
        "\n",
        "# Initialize the global toolkit\n",
        "AGENT_TOOLKIT = AgentRegistry()\n",
        "print(\"Agent Registry initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HzVg_IMC_yw",
        "outputId": "609a79de-b695-45eb-8be6-610b403b97ec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent Registry initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6.The Context Engine (Planner, Executor, Tracer)\n",
        "# -------------------------------------------------------------------------\n",
        "# This is the core innovation of Chapter 4. It replaces the linear\n",
        "# Orchestrator with a dynamic, LLM-driven planning and execution system.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# === 6.1. The Tracer (Debugging Implementation) ===\n",
        "class ExecutionTrace:\n",
        "    \"\"\"Logs the entire execution flow for debugging and analysis.\"\"\"\n",
        "    def __init__(self, goal):\n",
        "        self.goal = goal\n",
        "        self.plan = None\n",
        "        self.steps = []\n",
        "        self.status = \"Initialized\"\n",
        "        self.final_output = None\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_plan(self, plan):\n",
        "        self.plan = plan\n",
        "\n",
        "    def log_step(self, step_num, agent, planned_input, mcp_output, resolved_input):\n",
        "        \"\"\"Logs the details of a single execution step.\"\"\"\n",
        "        self.steps.append({\n",
        "            \"step\": step_num,\n",
        "            \"agent\": agent,\n",
        "             # The raw input definitions from the plan (including $$REFS$$)\n",
        "            \"planned_input\": planned_input,\n",
        "            # Crucial for debugging: What exact context did the agent receive?\n",
        "            \"resolved_context\": resolved_input,\n",
        "            \"output\": mcp_output['content']\n",
        "        })\n",
        "\n",
        "    def finalize(self, status, final_output=None):\n",
        "        self.status = status\n",
        "        self.final_output = final_output\n",
        "        self.duration = time.time() - self.start_time\n",
        "\n",
        "    def display_trace(self):\n",
        "        \"\"\"Displays the trace in a readable format.\"\"\"\n",
        "        display(Markdown(f\"### Execution Trace\\n**Goal:** {self.goal}\\n**Status:** {self.status} (Duration: {self.duration:.2f}s)\"))\n",
        "        if self.plan:\n",
        "            # Display the raw plan JSON\n",
        "            display(Markdown(f\"#### Plan:\\n```json\\n{json.dumps(self.plan, indent=2)}\\n```\"))\n",
        "\n",
        "        display(Markdown(\"#### Execution Steps:\"))\n",
        "        for step in self.steps:\n",
        "            print(f\"--- Step {step['step']}: {step['agent']} ---\")\n",
        "            print(\"  [Planned Input]:\", step['planned_input'])\n",
        "            # print(\"  [Resolved Context]:\", textwrap.shorten(str(step['resolved_context']), width=150))\n",
        "            print(\"  [Output Snippet]:\", textwrap.shorten(str(step['output']), width=150))\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# === 6.2. The Planner (Strategic Analysis) ===\n",
        "def planner(goal, capabilities):\n",
        "    \"\"\"\n",
        "    Analyzes the goal and generates a structured Execution Plan using the LLM.\n",
        "    \"\"\"\n",
        "    print(\"[Engine: Planner] Analyzing goal and generating execution plan...\")\n",
        "    system_prompt = f\"\"\"\n",
        "    You are the strategic core of the Context Engine. Analyze the user's high-level goal and create a structured Execution Plan using the available agents.\n",
        "\n",
        "    --- AVAILABLE CAPABILITIES ---\n",
        "    {capabilities}\n",
        "    --- END CAPABILITIES ---\n",
        "\n",
        "    INSTRUCTIONS:\n",
        "    1. The plan MUST be a JSON list of objects, where each object is a \"step\".\n",
        "    2. You MUST use Context Chaining. If a step requires input from a previous step, reference it using the syntax $$STEP_X_OUTPUT$$.\n",
        "    3. Be strategic. Break down complex goals (like sequential rewriting) into distinct steps. Use the correct input keys ('facts' vs 'previous_content') for the Writer agent.\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a suspenseful story about Apollo 11.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"suspenseful narrative blueprint\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Apollo 11 landing details\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    EXAMPLE GOAL: \"Write a technical report on Juno, then rewrite it casually.\"\n",
        "    EXAMPLE PLAN (JSON LIST):\n",
        "    [\n",
        "        {{\"step\": 1, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"technical report structure\"}}}},\n",
        "        {{\"step\": 2, \"agent\": \"Researcher\", \"input\": {{\"topic_query\": \"Juno mission technology\"}}}},\n",
        "        {{\"step\": 3, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_1_OUTPUT$$\", \"facts\": \"$$STEP_2_OUTPUT$$\"}}}},\n",
        "        {{\"step\": 4, \"agent\": \"Librarian\", \"input\": {{\"intent_query\": \"casual summary style\"}}}},\n",
        "        {{\"step\": 5, \"agent\": \"Writer\", \"input\": {{\"blueprint\": \"$$STEP_4_OUTPUT$$\", \"previous_content\": \"$$STEP_3_OUTPUT$$\"}}}}\n",
        "    ]\n",
        "\n",
        "    Respond ONLY with the JSON list.\n",
        "    \"\"\"\n",
        "    # Call LLM in JSON mode for reliability\n",
        "    plan_json = \"\"\n",
        "    try:\n",
        "        plan_json = call_llm_robust(system_prompt, goal, json_mode=True)\n",
        "        plan = json.loads(plan_json)\n",
        "\n",
        "        if not isinstance(plan, list):\n",
        "             # Handle cases where the LLM wraps the list in a dictionary\n",
        "             if isinstance(plan, dict):\n",
        "                 if \"plan\" in plan and isinstance(plan[\"plan\"], list):\n",
        "                     plan = plan[\"plan\"]\n",
        "                 elif \"steps\" in plan and isinstance(plan[\"steps\"], list): # <--- ADD THIS CHECK\n",
        "                     plan = plan[\"steps\"]\n",
        "                 else:\n",
        "                    raise ValueError(\"Planner returned a dict, but missing 'plan' or 'steps' key.\")\n",
        "             else:\n",
        "                raise ValueError(\"Planner did not return a valid JSON list structure.\")\n",
        "\n",
        "        print(\"[Engine: Planner] Plan generated successfully.\")\n",
        "        return plan\n",
        "    except Exception as e:\n",
        "        print(f\"[Engine: Planner] Failed to generate a valid plan. Error: {e}. Raw LLM Output: {plan_json}\")\n",
        "        raise e\n",
        "\n",
        "\n",
        "# === 6.3. The Executor (Context Assembly and Execution) ===\n",
        "\n",
        "def resolve_dependencies(input_params, state):\n",
        "    \"\"\"\n",
        "    Helper function to replace $$REF$$ placeholders with actual data from the execution state.\n",
        "    This implements Context Chaining.\n",
        "    \"\"\"\n",
        "    # Use copy.deepcopy to ensure the original plan structure is not modified\n",
        "    resolved_input = copy.deepcopy(input_params)\n",
        "\n",
        "    # Recursive function to handle potential nested structures\n",
        "    def resolve(value):\n",
        "        if isinstance(value, str) and value.startswith(\"$$\") and value.endswith(\"$$\"):\n",
        "            ref_key = value[2:-2]\n",
        "            if ref_key in state:\n",
        "                # Retrieve the actual data (string) from the previous step's output\n",
        "                print(f\"[Engine: Executor] Resolved dependency {ref_key}.\")\n",
        "                return state[ref_key]\n",
        "            else:\n",
        "                raise ValueError(f\"Dependency Error: Reference {ref_key} not found in execution state.\")\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: resolve(v) for k, v in value.items()}\n",
        "        elif isinstance(value, list):\n",
        "            return [resolve(v) for v in value]\n",
        "        return value\n",
        "\n",
        "    return resolve(resolved_input)\n",
        "\n",
        "\n",
        "def context_engine(goal):\n",
        "    \"\"\"\n",
        "    The main entry point for the Context Engine. Manages Planning and Execution.\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== [Context Engine] Starting New Task ===\\nGoal: {goal}\\n\")\n",
        "    trace = ExecutionTrace(goal)\n",
        "    registry = AGENT_TOOLKIT\n",
        "\n",
        "    # Phase 1: Plan\n",
        "    try:\n",
        "        capabilities = registry.get_capabilities_description()\n",
        "        plan = planner(goal, capabilities)\n",
        "        trace.log_plan(plan)\n",
        "    except Exception as e:\n",
        "        trace.finalize(\"Failed during Planning\")\n",
        "        # Return the trace even in failure for debugging\n",
        "        return None, trace\n",
        "\n",
        "    # Phase 2: Execute\n",
        "    # State stores the raw outputs (strings) of each step: { \"STEP_X_OUTPUT\": data_string }\n",
        "    state = {}\n",
        "\n",
        "    for step in plan:\n",
        "        step_num = step.get(\"step\")\n",
        "        agent_name = step.get(\"agent\")\n",
        "        planned_input = step.get(\"input\")\n",
        "\n",
        "        print(f\"\\n[Engine: Executor] Starting Step {step_num}: {agent_name}\")\n",
        "\n",
        "        try:\n",
        "            handler = registry.get_handler(agent_name)\n",
        "\n",
        "            # Context Assembly: Resolve dependencies\n",
        "            resolved_input = resolve_dependencies(planned_input, state)\n",
        "\n",
        "            # Execute Agent via MCP\n",
        "            # Create an MCP message with the RESOLVED input for the agent\n",
        "            mcp_resolved_input = create_mcp_message(\"Engine\", resolved_input)\n",
        "            mcp_output = handler(mcp_resolved_input)\n",
        "\n",
        "            # Update State and Log Trace\n",
        "            output_data = mcp_output[\"content\"]\n",
        "\n",
        "            # Store the output data (the context itself)\n",
        "            state[f\"STEP_{step_num}_OUTPUT\"] = output_data\n",
        "            trace.log_step(step_num, agent_name, planned_input, mcp_output, resolved_input)\n",
        "            print(f\"[Engine: Executor] Step {step_num} completed.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"Execution failed at step {step_num} ({agent_name}): {e}\"\n",
        "            print(f\"[Engine: Executor] ERROR: {error_message}\")\n",
        "            trace.finalize(f\"Failed at Step {step_num}\")\n",
        "            # Return the trace for debugging the failure\n",
        "            return None, trace\n",
        "\n",
        "    # Finalization\n",
        "    final_output = state.get(f\"STEP_{len(plan)}_OUTPUT\")\n",
        "    trace.finalize(\"Success\", final_output)\n",
        "    print(\"\\n=== [Context Engine] Task Complete ===\")\n",
        "\n",
        "    # Return the output of the final step AND the trace\n",
        "    return final_output, trace"
      ],
      "metadata": {
        "id": "SpHqskOtDHlZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 7.Execution (Standard Goal)\n",
        "# -------------------------------------------------------------------------\n",
        "# Demonstrate the engine with a standard goal similar to Chapter 3,\n",
        "# showing how the Planner dynamically constructs the workflow.\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "print(\"******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\\n\")\n",
        "goal_1 = \"Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\"\n",
        "\n",
        "# Run the Context Engine\n",
        "# Ensure the Pinecone index is populated (from Ch3 notebook) for this to work.\n",
        "result_1, trace_1 = context_engine(goal_1)\n",
        "\n",
        "if result_1:\n",
        "    print(\"\\n******** FINAL OUTPUT 1 **********\\n\")\n",
        "    display(Markdown(result_1))\n",
        "    print(\"\\n\\n\" + \"=\"*50 + \"\\n\\n\")\n",
        "    # Optional: Display the trace to see the engine's process\n",
        "    # trace_1.display_trace()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WB7udFkcDQBS",
        "outputId": "2481d262-fe77-426d-948e-57b04b073c79"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "******** Example 1: STANDARD WORKFLOW (Suspenseful Narrative) **********\n",
            "\n",
            "\n",
            "=== [Context Engine] Starting New Task ===\n",
            "Goal: Write a short, suspenseful scene for a children's story about the Apollo 11 moon landing, highlighting the danger.\n",
            "\n",
            "[Engine: Planner] Analyzing goal and generating execution plan...\n",
            "[Engine: Planner] Plan generated successfully.\n",
            "\n",
            "[Engine: Executor] Starting Step 1: Librarian\n",
            "\n",
            "[Librarian] Activated. Analyzing intent...\n",
            "[Librarian] Found blueprint 'blueprint_suspense_narrative' (Score: 0.66)\n",
            "[Engine: Executor] Step 1 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 2: Researcher\n",
            "\n",
            "[Researcher] Activated. Investigating topic...\n",
            "[Researcher] Found 2 relevant chunks. Synthesizing...\n",
            "[Engine: Executor] Step 2 completed.\n",
            "\n",
            "[Engine: Executor] Starting Step 3: Writer\n",
            "[Engine: Executor] Resolved dependency STEP_1_OUTPUT.\n",
            "[Engine: Executor] Resolved dependency STEP_2_OUTPUT.\n",
            "\n",
            "[Writer] Activated. Applying blueprint to source material...\n",
            "[Engine: Executor] Step 3 completed.\n",
            "\n",
            "=== [Context Engine] Task Complete ===\n",
            "\n",
            "******** FINAL OUTPUT 1 **********\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The dust hung in the Martian air like a thin, red fog.  \n\nThe Agent stood beside Perseverance. The rover barely moved. Just a low hum. A click. Another click. Each sound felt too loud in the stillness.\n\nThe sky above was a dull orange. No birds. No wind in trees. Just the dry whisper of sand sliding over rock.\n\nFar off, something shifted. Or maybe it was only the Agent’s mind. Mars loved to play tricks. Shapes in the distance curved and blurred in the dusty light. Shadows stretched long and thin, like fingers.\n\nPerseverance turned its camera. The lens glinted. It watched the empty land, searching for what used to live here. Ancient life. Long gone. Maybe not completely gone.\n\nThe Agent listened to the rover’s instruments. Soft beeps. A quiet buzz. Each new reading meant something hidden under the surface. Rocks. Ice. Maybe a trace of something that once breathed.\n\nThe Source_of_Threat waited in silence. It was not a monster or a creature. It was the unknown. The thin air that could not be breathed. The cold that could bite straight through metal. The dust that could creep into every crack and joint.\n\nThe rover drilled into the rock. The sound was sharp and dry, like bone breaking. Grit scraped against the metal bit. Tiny bits of ancient Mars curled up from the hole and floated down, slow as ash.\n\nThe Agent thought about the samples. Tiny tubes, filled with secrets. If they were ever brought back, people would open them and learn what this world had hidden for billions of years. But for now, the tubes rested in the soil, half-buried like little time bombs of knowledge.\n\nA low wind rose. Dust hissed along the ground. The sky dimmed. Shadows thickened around Perseverance’s wheels. The Agent watched the storm build, thin at first, then heavier, like a curtain drawing across the land.\n\nThe dust scraped the rover. Tap. Tap. Tap. Each tap was a warning. Dust could clog. Dust could blind. Dust could end the mission without a sound.\n\nFar above, Ingenuity waited. The small helicopter sat still, its legs pressed into the grit. The air was so thin that flying here felt almost impossible. One mistake, one wrong spin of its rotors, and it would tip, break, and never rise again.\n\nThe Agent sent the command. A beep. A pause. The longest pause in the world.\n\nThen the rotors moved. Slow at first. Faster. The sound was a sharp whir, like a swarm of metal insects. Dust lifted and danced around the little craft.\n\nFor a second, it did not lift. It only shuddered. The thin air pushed back. The Source_of_Threat—this harsh, alien sky—seemed to press down, to pin the helicopter in place.\n\nThen its shadow tore free of the ground. Ingenuity rose, just a little. The shadow shook on the red dirt, edges trembling. The helicopter hovered, a tiny, brave speck in a huge, uncaring world.\n\nEvery second in the air felt fragile. One gust. One glitch. One speck of dust in the wrong place. The Agent stared at the numbers on the screen. Altitude. Speed. Power. Each number climbed, then steadied. Each one could suddenly drop.\n\nThe air howled softly around the spinning blades. No one could hear it there. But in the quiet control room far away, everyone leaned forward and held their breath, listening to the silence that might become an alarm.\n\nIngenuity turned. Its camera looked down. It saw Perseverance below. The rover was a small, still shape surrounded by its own dark shadow. Beyond it, Mars stretched out in rust-red waves, empty and endless.\n\nThe helicopter began to descend. Slowly. Too slowly. The Agent watched the shadow grow darker and sharper as it neared the ground. One wrong angle, and a leg would snap. One slip, and it would topple.\n\nThe rotors slowed. The dust settled. The legs touched down with a soft, invisible thud.\n\nSilence again.\n\nThe Agent let out a breath. The Source_of_Threat did not. Mars stayed cold, thin, and watchful.\n\nNight crept in. The light faded, turning orange to deep red, then to purple, then almost black. The temperature dropped fast, like a stone. Metal shrank. Batteries drained.\n\nPerseverance sent one last stream of data. Clicks. Beeps. A faint, distant whisper through space. Then it grew still for the long, freezing dark.\n\nThe Agent could not see what moved out there in the shadows. There were no animals. No plants. Only wind. Only dust. Only the old, buried memory of life.\n\nBut that was enough. Enough to make every sound sharp. Every shadow strange. Every step in this quiet, alien place feel like the start of something waiting just beneath the surface, ready to be found—or not—one careful, tense moment at a time."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}